{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95. サブワード化\n",
    "トークンの単位を単語や形態素からサブワードに変更し，91-94の実験を再度実施せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU prepare\n",
    "1. 使用可能GPUの確認\n",
    "2. GPUの指定\n",
    "3. PyTorchで利用できるGPU数の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  5 16:51:21 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:01:00.0 Off |                  Off |\n",
      "| 30%   32C    P8    24W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    On   | 00000000:25:00.0 Off |                  Off |\n",
      "| 34%   63C    P2   176W / 300W |   3085MiB / 48685MiB |     41%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    On   | 00000000:41:00.0 Off |                  Off |\n",
      "| 30%   29C    P8    30W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    On   | 00000000:61:00.0 Off |                  Off |\n",
      "| 30%   28C    P8    20W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000    On   | 00000000:81:00.0 Off |                  Off |\n",
      "| 30%   28C    P8    19W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000    On   | 00000000:A1:00.0 Off |                  Off |\n",
      "| 30%   27C    P8    18W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000    On   | 00000000:C1:00.0 Off |                  Off |\n",
      "| 30%   28C    P8    16W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA RTX A6000    On   | 00000000:E1:00.0 Off |                  Off |\n",
      "| 30%   27C    P8    15W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# 使用可能GPUの確認\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUの指定\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' #0番を使用するとき"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 確認\n",
    "import torch\n",
    "print(torch.cuda.device_count()) #Pytorchで使用できるGPU数を取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トークン化 (90相当)\n",
    "1. 日本語のトークン化関数 (サブワード)\n",
    "2. 英語のトークン化関数 (サブワード)\n",
    "3. テキストのトークン化, 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=../data/ch10/kftt-data-1.0/data/orig/kyoto-train.ja --model_prefix=../data/ch10/95_sp_model_ja --vocab_size=16000 --character_coverage=0.9995 --pad_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/ch10/kftt-data-1.0/data/orig/kyoto-train.ja\n",
      "  input_format: \n",
      "  model_prefix: ../data/ch10/95_sp_model_ja\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ../data/ch10/kftt-data-1.0/data/orig/kyoto-train.ja\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 440288 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=17202261\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=3975\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 440288 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 1000000 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 440288\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 427559\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 427559 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=575029 obj=152.017 num_tokens=6375002 num_tokens/piece=11.0864\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=493763 obj=137.525 num_tokens=6425021 num_tokens/piece=13.0124\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=369500 obj=137.732 num_tokens=6564735 num_tokens/piece=17.7665\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=367852 obj=137.219 num_tokens=6593593 num_tokens/piece=17.9246\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=275766 obj=138.643 num_tokens=6766779 num_tokens/piece=24.5381\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=275627 obj=138.087 num_tokens=6770942 num_tokens/piece=24.5656\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=206714 obj=140.22 num_tokens=6988104 num_tokens/piece=33.8057\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=206700 obj=139.556 num_tokens=6993001 num_tokens/piece=33.8316\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=155024 obj=142.256 num_tokens=7232760 num_tokens/piece=46.6557\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=155024 obj=141.549 num_tokens=7237855 num_tokens/piece=46.6886\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=116268 obj=144.734 num_tokens=7494616 num_tokens/piece=64.4598\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=116268 obj=144.02 num_tokens=7497296 num_tokens/piece=64.4829\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=87201 obj=147.568 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_tokens=7770269 num_tokens/piece=89.1076\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=87201 obj=146.853 num_tokens=7771254 num_tokens/piece=89.1189\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=65400 obj=150.704 num_tokens=8061533 num_tokens/piece=123.265\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=65400 obj=149.975 num_tokens=8063918 num_tokens/piece=123.301\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=49050 obj=154.078 num_tokens=8376373 num_tokens/piece=170.772\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=49050 obj=153.295 num_tokens=8377709 num_tokens/piece=170.799\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=36787 obj=157.671 num_tokens=8711083 num_tokens/piece=236.798\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=36787 obj=156.827 num_tokens=8711831 num_tokens/piece=236.818\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=27590 obj=161.517 num_tokens=9079741 num_tokens/piece=329.095\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=27590 obj=160.583 num_tokens=9080761 num_tokens/piece=329.132\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=20692 obj=165.671 num_tokens=9490526 num_tokens/piece=458.657\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=20692 obj=164.644 num_tokens=9491072 num_tokens/piece=458.683\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=167.715 num_tokens=9740378 num_tokens/piece=553.431\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=167.109 num_tokens=9741284 num_tokens/piece=553.482\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: ../data/ch10/95_sp_model_ja.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: ../data/ch10/95_sp_model_ja.vocab\n"
     ]
    }
   ],
   "source": [
    "# 日本語のサブワード分割\n",
    "import sentencepiece as spm\n",
    "\n",
    "# サブワード分割のTrain\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input=../data/ch10/kftt-data-1.0/data/orig/kyoto-train.ja --model_prefix=../data/ch10/95_sp_model_ja --vocab_size=16000 --character_coverage=0.9995 --pad_id=3'\n",
    ")\n",
    "\n",
    "# 日本語のサブワード分割用インスタンス\n",
    "sp_ja = spm.SentencePieceProcessor()\n",
    "\n",
    "# Trainしたモデルをロード\n",
    "sp_ja.Load('../data/ch10/95_sp_model_ja.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=../data/ch10/kftt-data-1.0/data/orig/kyoto-train.en --model_prefix=../data/ch10/95_sp_model_en --vocab_size=16000 --character_coverage=0.9995 --pad_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/ch10/kftt-data-1.0/data/orig/kyoto-train.en\n",
      "  input_format: \n",
      "  model_prefix: ../data/ch10/95_sp_model_en\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ../data/ch10/kftt-data-1.0/data/orig/kyoto-train.en\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 440286 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=59809000\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=262\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 440286 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 405729 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 440286\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 453765\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 453765 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=170397 obj=11.7026 num_tokens=1089069 num_tokens/piece=6.39136\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=146188 obj=9.20893 num_tokens=1096873 num_tokens/piece=7.50317\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=109608 obj=9.17804 num_tokens=1133933 num_tokens/piece=10.3453\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=109491 obj=9.16716 num_tokens=1139992 num_tokens/piece=10.4117\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=82116 obj=9.21286 num_tokens=1194673 num_tokens/piece=14.5486\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=82112 obj=9.20361 num_tokens=1195313 num_tokens/piece=14.5571\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61583 obj=9.27077 num_tokens=1258055 num_tokens/piece=20.4286\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61583 obj=9.25723 num_tokens=1258068 num_tokens/piece=20.4288\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46187 obj=9.35145 num_tokens=1324679 num_tokens/piece=28.6808\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46187 obj=9.33413 num_tokens=1324652 num_tokens/piece=28.6802\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34640 obj=9.45344 num_tokens=1393117 num_tokens/piece=40.217\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34640 obj=9.43117 num_tokens=1393145 num_tokens/piece=40.2178\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25980 obj=9.58089 num_tokens=1461254 num_tokens/piece=56.2453\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25980 obj=9.55338 num_tokens=1461208 num_tokens/piece=56.2436\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19485 obj=9.73507 num_tokens=1529034 num_tokens/piece=78.4724\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19485 obj=9.70182 num_tokens=1529107 num_tokens/piece=78.4761\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=9.77045 num_tokens=1553130 num_tokens/piece=88.246\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=9.75856 num_tokens=1553172 num_tokens/piece=88.2484\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: ../data/ch10/95_sp_model_en.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: ../data/ch10/95_sp_model_en.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 英語のサブワード分割\n",
    "# サブワード分割のTrain\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input=../data/ch10/kftt-data-1.0/data/orig/kyoto-train.en --model_prefix=../data/ch10/95_sp_model_en --vocab_size=16000 --character_coverage=0.9995 --pad_id=3'\n",
    ")\n",
    "\n",
    "# 英語のサブワード分割用インスタンス\n",
    "sp_en = spm.SentencePieceProcessor()\n",
    "\n",
    "# Trainしたモデルをロード\n",
    "sp_en.Load('../data/ch10/95_sp_model_en.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストをトークン列に変換, 保存\n",
    "import torch\n",
    "\n",
    "def tokenizer(fname_org, fname_tok, sp):\n",
    "    with open(fname_org, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    tokens = [' '.join(sp.EncodeAsPieces(line)) + '\\n' for line in lines]\n",
    "\n",
    "    with open(fname_tok, 'w') as f:\n",
    "        f.writelines(tokens)\n",
    "\n",
    "tokenizer('../data/ch10/kftt-data-1.0/data/orig/kyoto-train.ja', '../data/ch10/95_train_tokens.ja', sp_ja)\n",
    "tokenizer('../data/ch10/kftt-data-1.0/data/orig/kyoto-dev.ja', '../data/ch10/95_dev_tokens.ja', sp_ja)\n",
    "tokenizer('../data/ch10/kftt-data-1.0/data/orig/kyoto-test.ja', '../data/ch10/95_dev_test.ja', sp_ja)\n",
    "tokenizer('../data/ch10/kftt-data-1.0/data/orig/kyoto-train.en', '../data/ch10/95_train_tokens.en', sp_en)\n",
    "tokenizer('../data/ch10/kftt-data-1.0/data/orig/kyoto-dev.en', '../data/ch10/95_dev_tokens.en', sp_en)\n",
    "tokenizer('../data/ch10/kftt-data-1.0/data/orig/kyoto-test.en', '../data/ch10/95_dev_test.en', sp_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 翻訳モデルの訓練 (91相当)\n",
    "1. データの前処理\n",
    "2. モデルの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-05 17:14:24 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 17:14:24 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='../data/ch10/95_preprocessed', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='ja', srcdict=None, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=5, thresholdtgt=5, tokenizer=None, tpu=False, trainpref='../data/ch10/95_train_tokens', use_plasma_view=False, user_dir=None, validpref='../data/ch10/95_dev_tokens', wandb_project=None, workers=20)\n",
      "2022-08-05 17:14:30 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 16632 types\n",
      "2022-08-05 17:14:40 | INFO | fairseq_cli.preprocess | [ja] ../data/ch10/95_train_tokens.ja: 440288 sents, 10462353 tokens, 0.0284% replaced (by <unk>)\n",
      "2022-08-05 17:14:40 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 16632 types\n",
      "2022-08-05 17:14:40 | INFO | fairseq_cli.preprocess | [ja] ../data/ch10/95_dev_tokens.ja: 1166 sents, 24223 tokens, 0.0289% replaced (by <unk>)\n",
      "2022-08-05 17:14:40 | INFO | fairseq_cli.preprocess | [en] Dictionary: 16784 types\n",
      "2022-08-05 17:14:50 | INFO | fairseq_cli.preprocess | [en] ../data/ch10/95_train_tokens.en: 440288 sents, 13803045 tokens, 0.0691% replaced (by <unk>)\n",
      "2022-08-05 17:14:50 | INFO | fairseq_cli.preprocess | [en] Dictionary: 16784 types\n",
      "2022-08-05 17:14:50 | INFO | fairseq_cli.preprocess | [en] ../data/ch10/95_dev_tokens.en: 1166 sents, 30076 tokens, 0.0199% replaced (by <unk>)\n",
      "2022-08-05 17:14:50 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ../data/ch10/95_preprocessed\n"
     ]
    }
   ],
   "source": [
    "# 前処理\n",
    "!fairseq-preprocess -s ja -t en \\\n",
    "    --trainpref ../data/ch10/95_train_tokens \\\n",
    "    --validpref ../data/ch10/95_dev_tokens \\\n",
    "    --destdir ../data/ch10/95_preprocessed \\\n",
    "    --thresholdsrc 5 \\\n",
    "    --thresholdtgt 5 \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-05 17:25:12 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 17:25:14 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 8000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 8000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 20, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '../data/ch10/95_trained', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../data/ch10/95_preprocessed', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.001], lr_scheduler='inverse_sqrt', max_epoch=20, max_tokens=8000, max_tokens_valid=8000, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='../data/ch10/95_trained', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=2000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2000, 'warmup_init_lr': 1e-07, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 17:25:14 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 17:25:14 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 17:25:15 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(16632, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(16784, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=16784, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-08-05 17:25:15 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-08-05 17:25:15 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-08-05 17:25:15 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-08-05 17:25:15 | INFO | fairseq_cli.train | num. shared model params: 80,149,504 (num. trained: 80,149,504)\n",
      "2022-08-05 17:25:15 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-08-05 17:25:15 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: ../data/ch10/95_preprocessed/valid.ja-en.ja\n",
      "2022-08-05 17:25:15 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: ../data/ch10/95_preprocessed/valid.ja-en.en\n",
      "2022-08-05 17:25:15 | INFO | fairseq.tasks.translation | ../data/ch10/95_preprocessed valid ja-en 1166 examples\n",
      "2022-08-05 17:25:16 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2022-08-05 17:25:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-05 17:25:16 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.544 GB ; name = NVIDIA RTX A6000                        \n",
      "2022-08-05 17:25:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-05 17:25:16 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-08-05 17:25:16 | INFO | fairseq_cli.train | max tokens per device = 8000 and max sentences per device = None\n",
      "2022-08-05 17:25:16 | INFO | fairseq.trainer | Preparing to load checkpoint ../data/ch10/95_trained/checkpoint_last.pt\n",
      "2022-08-05 17:25:16 | INFO | fairseq.trainer | No existing checkpoint found ../data/ch10/95_trained/checkpoint_last.pt\n",
      "2022-08-05 17:25:16 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-08-05 17:25:16 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: ../data/ch10/95_preprocessed/train.ja-en.ja\n",
      "2022-08-05 17:25:16 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: ../data/ch10/95_preprocessed/train.ja-en.en\n",
      "2022-08-05 17:25:16 | INFO | fairseq.tasks.translation | ../data/ch10/95_preprocessed train ja-en 440288 examples\n",
      "2022-08-05 17:25:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 001:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 17:25:17 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-08-05 17:25:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2022-08-05 17:25:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   1%|▏                             | 15/1935 [00:02<03:04, 10.40it/s]2022-08-05 17:25:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:   1%|▍                             | 27/1935 [00:03<02:43, 11.67it/s]2022-08-05 17:25:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:  13%|▏| 259/1935 [00:22<02:25, 11.49it/s, loss=10.773, nll_loss=10.262022-08-05 17:25:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001: 100%|▉| 1933/1935 [02:41<00:00, 11.98it/s, loss=7.623, nll_loss=6.5542022-08-05 17:27:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  11%|▉       | 1/9 [00:00<00:01,  6.75it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  67%|█████▎  | 6/9 [00:00<00:00, 25.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 17:27:59 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.661 | nll_loss 6.546 | ppl 93.46 | wps 158893 | wpb 3341.8 | bsz 129.6 | num_updates 1931\n",
      "2022-08-05 17:27:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1931 updates\n",
      "2022-08-05 17:27:59 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint1.pt\n",
      "2022-08-05 17:28:00 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint1.pt\n",
      "2022-08-05 17:28:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint1.pt (epoch 1 @ 1931 updates, score 7.661) (writing took 2.0076686709653586 seconds)\n",
      "2022-08-05 17:28:01 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-08-05 17:28:01 | INFO | train | epoch 001 | loss 8.699 | nll_loss 7.82 | ppl 225.91 | wps 84574.7 | ups 11.86 | wpb 7133.9 | bsz 225.6 | num_updates 1931 | lr 0.000965503 | gnorm 1.182 | clip 46.8 | loss_scale 8 | train_wall 158 | gb_free 43.3 | wall 164\n",
      "2022-08-05 17:28:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 002:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 17:28:01 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-08-05 17:28:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002: 100%|▉| 1933/1935 [02:39<00:00, 12.52it/s, loss=6.813, nll_loss=5.6292022-08-05 17:30:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  44%|███▌    | 4/9 [00:00<00:00, 33.25it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 9/9 [00:00<00:00, 41.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 17:30:41 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.045 | nll_loss 5.854 | ppl 57.83 | wps 159920 | wpb 3341.8 | bsz 129.6 | num_updates 3866 | best_loss 7.045\n",
      "2022-08-05 17:30:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3866 updates\n",
      "2022-08-05 17:30:41 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint2.pt\n",
      "2022-08-05 17:30:42 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint2.pt\n",
      "2022-08-05 17:30:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint2.pt (epoch 2 @ 3866 updates, score 7.045) (writing took 2.979169708909467 seconds)\n",
      "2022-08-05 17:30:44 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-08-05 17:30:44 | INFO | train | epoch 002 | loss 7.092 | nll_loss 5.948 | ppl 61.75 | wps 84787.5 | ups 11.89 | wpb 7133.4 | bsz 227.5 | num_updates 3866 | lr 0.000719257 | gnorm 0.592 | clip 4.1 | loss_scale 8 | train_wall 156 | gb_free 43.8 | wall 327\n",
      "2022-08-05 17:30:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 003:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 17:30:44 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-08-05 17:30:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003: 100%|▉| 1933/1935 [02:40<00:00, 12.42it/s, loss=6.548, nll_loss=5.3232022-08-05 17:33:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  33%|██▋     | 3/9 [00:00<00:00, 29.03it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  89%|███████ | 8/9 [00:00<00:00, 37.22it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 17:33:25 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.851 | nll_loss 5.598 | ppl 48.44 | wps 157097 | wpb 3341.8 | bsz 129.6 | num_updates 5801 | best_loss 6.851\n",
      "2022-08-05 17:33:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5801 updates\n",
      "2022-08-05 17:33:25 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint3.pt\n",
      "2022-08-05 17:33:26 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint3.pt\n",
      "2022-08-05 17:33:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint3.pt (epoch 3 @ 5801 updates, score 6.851) (writing took 2.9894342650659382 seconds)\n",
      "2022-08-05 17:33:28 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-08-05 17:33:28 | INFO | train | epoch 003 | loss 6.608 | nll_loss 5.391 | ppl 41.97 | wps 84161.4 | ups 11.8 | wpb 7133.4 | bsz 227.5 | num_updates 5801 | lr 0.00058717 | gnorm 0.566 | clip 3.4 | loss_scale 8 | train_wall 157 | gb_free 43.3 | wall 491\n",
      "2022-08-05 17:33:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 004:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 17:33:28 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-08-05 17:33:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004: 100%|▉| 1933/1935 [02:40<00:00, 12.02it/s, loss=6.2, nll_loss=4.924, 2022-08-05 17:36:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  33%|██▋     | 3/9 [00:00<00:00, 29.56it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  89%|███████ | 8/9 [00:00<00:00, 39.06it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 17:36:08 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.728 | nll_loss 5.456 | ppl 43.9 | wps 160792 | wpb 3341.8 | bsz 129.6 | num_updates 7736 | best_loss 6.728\n",
      "2022-08-05 17:36:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 7736 updates\n",
      "2022-08-05 17:36:08 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint4.pt\n",
      "2022-08-05 17:36:09 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint4.pt\n",
      "2022-08-05 17:36:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint4.pt (epoch 4 @ 7736 updates, score 6.728) (writing took 2.9824079440440983 seconds)\n",
      "2022-08-05 17:36:11 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-08-05 17:36:11 | INFO | train | epoch 004 | loss 6.347 | nll_loss 5.091 | ppl 34.08 | wps 84347.7 | ups 11.82 | wpb 7133.4 | bsz 227.5 | num_updates 7736 | lr 0.00050846 | gnorm 0.574 | clip 3.4 | loss_scale 8 | train_wall 156 | gb_free 43.4 | wall 655\n",
      "2022-08-05 17:36:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 005:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 17:36:11 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-08-05 17:36:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005: 100%|▉| 1933/1935 [02:39<00:00, 11.98it/s, loss=6.158, nll_loss=4.8742022-08-05 17:38:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  44%|███▌    | 4/9 [00:00<00:00, 34.41it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 9/9 [00:00<00:00, 41.19it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 17:38:52 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.674 | nll_loss 5.393 | ppl 42.03 | wps 160578 | wpb 3341.8 | bsz 129.6 | num_updates 9671 | best_loss 6.674\n",
      "2022-08-05 17:38:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 9671 updates\n",
      "2022-08-05 17:38:52 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint5.pt\n",
      "2022-08-05 17:38:53 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint5.pt\n",
      "2022-08-05 17:38:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint5.pt (epoch 5 @ 9671 updates, score 6.674) (writing took 2.9832861989270896 seconds)\n",
      "2022-08-05 17:38:55 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-08-05 17:38:55 | INFO | train | epoch 005 | loss 6.174 | nll_loss 4.891 | ppl 29.67 | wps 84446.7 | ups 11.84 | wpb 7133.4 | bsz 227.5 | num_updates 9671 | lr 0.000454757 | gnorm 0.587 | clip 3.4 | loss_scale 8 | train_wall 156 | gb_free 43.4 | wall 818\n",
      "2022-08-05 17:38:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 006:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 17:38:55 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2022-08-05 17:38:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006: 100%|▉| 1933/1935 [02:38<00:00, 12.57it/s, loss=6.108, nll_loss=4.8142022-08-05 17:41:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  44%|███▌    | 4/9 [00:00<00:00, 34.52it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 9/9 [00:00<00:00, 42.45it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 17:41:34 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.626 | nll_loss 5.336 | ppl 40.4 | wps 164506 | wpb 3341.8 | bsz 129.6 | num_updates 11606 | best_loss 6.626\n",
      "2022-08-05 17:41:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 11606 updates\n",
      "2022-08-05 17:41:34 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint6.pt\n",
      "2022-08-05 17:41:35 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint6.pt\n",
      "2022-08-05 17:41:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint6.pt (epoch 6 @ 11606 updates, score 6.626) (writing took 3.004725035978481 seconds)\n",
      "2022-08-05 17:41:37 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2022-08-05 17:41:37 | INFO | train | epoch 006 | loss 6.044 | nll_loss 4.74 | ppl 26.72 | wps 85022.6 | ups 11.92 | wpb 7133.4 | bsz 227.5 | num_updates 11606 | lr 0.00041512 | gnorm 0.592 | clip 3.8 | loss_scale 8 | train_wall 155 | gb_free 43.4 | wall 981\n",
      "2022-08-05 17:41:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 007:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 17:41:37 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2022-08-05 17:41:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007: 100%|▉| 1933/1935 [02:38<00:00, 11.71it/s, loss=5.981, nll_loss=4.6672022-08-05 17:44:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  33%|██▋     | 3/9 [00:00<00:00, 25.73it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  89%|███████ | 8/9 [00:00<00:00, 35.84it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 17:44:16 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.582 | nll_loss 5.288 | ppl 39.08 | wps 159730 | wpb 3341.8 | bsz 129.6 | num_updates 13541 | best_loss 6.582\n",
      "2022-08-05 17:44:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 13541 updates\n",
      "2022-08-05 17:44:16 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint7.pt\n",
      "2022-08-05 17:44:18 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint7.pt\n",
      "2022-08-05 17:44:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint7.pt (epoch 7 @ 13541 updates, score 6.582) (writing took 2.9794032170902938 seconds)\n",
      "2022-08-05 17:44:19 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2022-08-05 17:44:19 | INFO | train | epoch 007 | loss 5.944 | nll_loss 4.624 | ppl 24.66 | wps 84979.4 | ups 11.91 | wpb 7133.4 | bsz 227.5 | num_updates 13541 | lr 0.000384317 | gnorm 0.597 | clip 3.4 | loss_scale 8 | train_wall 155 | gb_free 43.4 | wall 1143\n",
      "2022-08-05 17:44:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 008:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 17:44:20 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2022-08-05 17:44:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008: 100%|▉| 1933/1935 [02:40<00:00, 12.59it/s, loss=5.892, nll_loss=4.5652022-08-05 17:47:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  44%|███▌    | 4/9 [00:00<00:00, 33.18it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 9/9 [00:00<00:00, 41.75it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 17:47:00 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.538 | nll_loss 5.227 | ppl 37.45 | wps 166377 | wpb 3341.8 | bsz 129.6 | num_updates 15476 | best_loss 6.538\n",
      "2022-08-05 17:47:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 15476 updates\n",
      "2022-08-05 17:47:00 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint8.pt\n",
      "2022-08-05 17:47:02 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint8.pt\n",
      "2022-08-05 17:47:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint8.pt (epoch 8 @ 15476 updates, score 6.538) (writing took 2.9756248551420867 seconds)\n",
      "2022-08-05 17:47:03 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2022-08-05 17:47:03 | INFO | train | epoch 008 | loss 5.861 | nll_loss 4.528 | ppl 23.08 | wps 84177.1 | ups 11.8 | wpb 7133.4 | bsz 227.5 | num_updates 15476 | lr 0.000359489 | gnorm 0.597 | clip 3.4 | loss_scale 8 | train_wall 157 | gb_free 43.1 | wall 1307\n",
      "2022-08-05 17:47:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 009:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 17:47:03 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2022-08-05 17:47:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009:  81%|▊| 1563/1935 [02:08<00:31, 11.83it/s, loss=5.718, nll_loss=4.3642022-08-05 17:49:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 009: 100%|▉| 1933/1935 [02:38<00:00, 12.26it/s, loss=5.737, nll_loss=4.3872022-08-05 17:49:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  44%|███▌    | 4/9 [00:00<00:00, 31.92it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 9/9 [00:00<00:00, 41.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 17:49:43 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.485 | nll_loss 5.164 | ppl 35.86 | wps 158835 | wpb 3341.8 | bsz 129.6 | num_updates 17410 | best_loss 6.485\n",
      "2022-08-05 17:49:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 17410 updates\n",
      "2022-08-05 17:49:43 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint9.pt\n",
      "2022-08-05 17:49:44 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint9.pt\n",
      "2022-08-05 17:49:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint9.pt (epoch 9 @ 17410 updates, score 6.485) (writing took 2.9815674191340804 seconds)\n",
      "2022-08-05 17:49:46 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2022-08-05 17:49:46 | INFO | train | epoch 009 | loss 5.792 | nll_loss 4.447 | ppl 21.82 | wps 84949.2 | ups 11.91 | wpb 7133.6 | bsz 226.3 | num_updates 17410 | lr 0.000338934 | gnorm 0.603 | clip 3.5 | loss_scale 8 | train_wall 155 | gb_free 43.3 | wall 1470\n",
      "2022-08-05 17:49:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 010:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 17:49:46 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2022-08-05 17:49:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010:  70%|▋| 1363/1935 [01:51<00:47, 12.04it/s, loss=5.714, nll_loss=4.3582022-08-05 17:51:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
      "epoch 010: 100%|▉| 1933/1935 [02:38<00:00, 12.78it/s, loss=5.766, nll_loss=4.4182022-08-05 17:52:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  33%|██▋     | 3/9 [00:00<00:00, 29.03it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  89%|███████ | 8/9 [00:00<00:00, 38.95it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 17:52:25 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.501 | nll_loss 5.185 | ppl 36.38 | wps 164954 | wpb 3341.8 | bsz 129.6 | num_updates 19344 | best_loss 6.485\n",
      "2022-08-05 17:52:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 19344 updates\n",
      "2022-08-05 17:52:25 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint10.pt\n",
      "2022-08-05 17:52:26 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint10.pt\n",
      "2022-08-05 17:52:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint10.pt (epoch 10 @ 19344 updates, score 6.501) (writing took 2.1767998561263084 seconds)\n",
      "2022-08-05 17:52:27 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2022-08-05 17:52:27 | INFO | train | epoch 010 | loss 5.732 | nll_loss 4.378 | ppl 20.79 | wps 85646 | ups 12.01 | wpb 7132.9 | bsz 227.4 | num_updates 19344 | lr 0.000321545 | gnorm 0.607 | clip 3.6 | loss_scale 4 | train_wall 155 | gb_free 43.4 | wall 1631\n",
      "2022-08-05 17:52:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 011:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 17:52:27 | INFO | fairseq.trainer | begin training epoch 11\n",
      "2022-08-05 17:52:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 011: 100%|▉| 1933/1935 [02:38<00:00, 12.33it/s, loss=5.685, nll_loss=4.3242022-08-05 17:55:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 011 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  44%|███▌    | 4/9 [00:00<00:00, 32.73it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 17:55:06 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.469 | nll_loss 5.145 | ppl 35.39 | wps 168925 | wpb 3341.8 | bsz 129.6 | num_updates 21279 | best_loss 6.469\n",
      "2022-08-05 17:55:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 21279 updates\n",
      "2022-08-05 17:55:06 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint11.pt\n",
      "2022-08-05 17:55:08 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint11.pt\n",
      "2022-08-05 17:55:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint11.pt (epoch 11 @ 21279 updates, score 6.469) (writing took 2.94269926706329 seconds)\n",
      "2022-08-05 17:55:09 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
      "2022-08-05 17:55:09 | INFO | train | epoch 011 | loss 5.679 | nll_loss 4.316 | ppl 19.92 | wps 84972.4 | ups 11.91 | wpb 7133.4 | bsz 227.5 | num_updates 21279 | lr 0.000306577 | gnorm 0.62 | clip 3.8 | loss_scale 4 | train_wall 155 | gb_free 43 | wall 1793\n",
      "2022-08-05 17:55:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 012:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 17:55:09 | INFO | fairseq.trainer | begin training epoch 12\n",
      "2022-08-05 17:55:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 012: 100%|▉| 1933/1935 [02:39<00:00, 12.70it/s, loss=5.684, nll_loss=4.3222022-08-05 17:57:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 012 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  44%|███▌    | 4/9 [00:00<00:00, 32.13it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 17:57:49 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.445 | nll_loss 5.117 | ppl 34.7 | wps 162294 | wpb 3341.8 | bsz 129.6 | num_updates 23214 | best_loss 6.445\n",
      "2022-08-05 17:57:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 23214 updates\n",
      "2022-08-05 17:57:49 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint12.pt\n",
      "2022-08-05 17:57:50 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint12.pt\n",
      "2022-08-05 17:57:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint12.pt (epoch 12 @ 23214 updates, score 6.445) (writing took 2.9455463299527764 seconds)\n",
      "2022-08-05 17:57:52 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
      "2022-08-05 17:57:52 | INFO | train | epoch 012 | loss 5.632 | nll_loss 4.261 | ppl 19.17 | wps 84926.6 | ups 11.91 | wpb 7133.4 | bsz 227.5 | num_updates 23214 | lr 0.000293522 | gnorm 0.616 | clip 3.9 | loss_scale 4 | train_wall 155 | gb_free 43.1 | wall 1956\n",
      "2022-08-05 17:57:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 013:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 17:57:52 | INFO | fairseq.trainer | begin training epoch 13\n",
      "2022-08-05 17:57:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 013: 100%|▉| 1933/1935 [02:38<00:00, 12.25it/s, loss=5.576, nll_loss=4.1972022-08-05 18:00:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 013 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  44%|███▌    | 4/9 [00:00<00:00, 32.16it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 18:00:31 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.384 | nll_loss 5.044 | ppl 33 | wps 160245 | wpb 3341.8 | bsz 129.6 | num_updates 25149 | best_loss 6.384\n",
      "2022-08-05 18:00:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 25149 updates\n",
      "2022-08-05 18:00:31 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint13.pt\n",
      "2022-08-05 18:00:33 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint13.pt\n",
      "2022-08-05 18:00:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint13.pt (epoch 13 @ 25149 updates, score 6.384) (writing took 2.9762417401652783 seconds)\n",
      "2022-08-05 18:00:34 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
      "2022-08-05 18:00:34 | INFO | train | epoch 013 | loss 5.59 | nll_loss 4.212 | ppl 18.53 | wps 84943.4 | ups 11.91 | wpb 7133.4 | bsz 227.5 | num_updates 25149 | lr 0.000282004 | gnorm 0.621 | clip 4 | loss_scale 4 | train_wall 155 | gb_free 43.2 | wall 2118\n",
      "2022-08-05 18:00:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 014:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 18:00:34 | INFO | fairseq.trainer | begin training epoch 14\n",
      "2022-08-05 18:00:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 014: 100%|▉| 1933/1935 [02:39<00:00, 12.37it/s, loss=5.576, nll_loss=4.1972022-08-05 18:03:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 014 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  44%|███▌    | 4/9 [00:00<00:00, 35.28it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset: 100%|████████| 9/9 [00:00<00:00, 42.64it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 18:03:14 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.364 | nll_loss 5.023 | ppl 32.52 | wps 164971 | wpb 3341.8 | bsz 129.6 | num_updates 27084 | best_loss 6.364\n",
      "2022-08-05 18:03:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 27084 updates\n",
      "2022-08-05 18:03:14 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint14.pt\n",
      "2022-08-05 18:03:15 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint14.pt\n",
      "2022-08-05 18:03:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint14.pt (epoch 14 @ 27084 updates, score 6.364) (writing took 2.943555205129087 seconds)\n",
      "2022-08-05 18:03:17 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
      "2022-08-05 18:03:17 | INFO | train | epoch 014 | loss 5.551 | nll_loss 4.167 | ppl 17.96 | wps 84778.3 | ups 11.88 | wpb 7133.4 | bsz 227.5 | num_updates 27084 | lr 0.000271743 | gnorm 0.627 | clip 3.8 | loss_scale 4 | train_wall 156 | gb_free 43.3 | wall 2281\n",
      "2022-08-05 18:03:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 015:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 18:03:17 | INFO | fairseq.trainer | begin training epoch 15\n",
      "2022-08-05 18:03:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 015: 100%|▉| 1933/1935 [02:38<00:00, 12.37it/s, loss=5.543, nll_loss=4.1572022-08-05 18:05:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 015 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  44%|███▌    | 4/9 [00:00<00:00, 34.07it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset: 100%|████████| 9/9 [00:00<00:00, 42.41it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 18:05:56 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.374 | nll_loss 5.026 | ppl 32.58 | wps 165496 | wpb 3341.8 | bsz 129.6 | num_updates 29019 | best_loss 6.364\n",
      "2022-08-05 18:05:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 29019 updates\n",
      "2022-08-05 18:05:56 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint15.pt\n",
      "2022-08-05 18:05:58 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint15.pt\n",
      "2022-08-05 18:05:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint15.pt (epoch 15 @ 29019 updates, score 6.374) (writing took 2.1822292041033506 seconds)\n",
      "2022-08-05 18:05:59 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
      "2022-08-05 18:05:59 | INFO | train | epoch 015 | loss 5.516 | nll_loss 4.125 | ppl 17.44 | wps 85458.3 | ups 11.98 | wpb 7133.4 | bsz 227.5 | num_updates 29019 | lr 0.000262527 | gnorm 0.626 | clip 3.7 | loss_scale 4 | train_wall 155 | gb_free 43.3 | wall 2442\n",
      "2022-08-05 18:05:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 016:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 18:05:59 | INFO | fairseq.trainer | begin training epoch 16\n",
      "2022-08-05 18:05:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 016: 100%|▉| 1933/1935 [02:40<00:00, 12.05it/s, loss=5.542, nll_loss=4.1552022-08-05 18:08:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 016 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  33%|██▋     | 3/9 [00:00<00:00, 27.31it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  89%|███████ | 8/9 [00:00<00:00, 38.28it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 18:08:39 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.341 | nll_loss 4.989 | ppl 31.75 | wps 161970 | wpb 3341.8 | bsz 129.6 | num_updates 30954 | best_loss 6.341\n",
      "2022-08-05 18:08:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 30954 updates\n",
      "2022-08-05 18:08:39 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint16.pt\n",
      "2022-08-05 18:08:41 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint16.pt\n",
      "2022-08-05 18:08:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint16.pt (epoch 16 @ 30954 updates, score 6.341) (writing took 2.9748308300040662 seconds)\n",
      "2022-08-05 18:08:42 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
      "2022-08-05 18:08:42 | INFO | train | epoch 016 | loss 5.485 | nll_loss 4.089 | ppl 17.02 | wps 84274.9 | ups 11.81 | wpb 7133.4 | bsz 227.5 | num_updates 30954 | lr 0.000254189 | gnorm 0.633 | clip 3.7 | loss_scale 4 | train_wall 157 | gb_free 43.3 | wall 2606\n",
      "2022-08-05 18:08:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 017:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 18:08:43 | INFO | fairseq.trainer | begin training epoch 17\n",
      "2022-08-05 18:08:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 017: 100%|▉| 1933/1935 [02:39<00:00, 11.73it/s, loss=5.459, nll_loss=4.0582022-08-05 18:11:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 017 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  44%|███▌    | 4/9 [00:00<00:00, 32.70it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 18:11:22 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.357 | nll_loss 5.005 | ppl 32.11 | wps 165549 | wpb 3341.8 | bsz 129.6 | num_updates 32889 | best_loss 6.341\n",
      "2022-08-05 18:11:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 32889 updates\n",
      "2022-08-05 18:11:22 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint17.pt\n",
      "2022-08-05 18:11:24 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint17.pt\n",
      "2022-08-05 18:11:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint17.pt (epoch 17 @ 32889 updates, score 6.357) (writing took 2.167784294113517 seconds)\n",
      "2022-08-05 18:11:24 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
      "2022-08-05 18:11:24 | INFO | train | epoch 017 | loss 5.455 | nll_loss 4.053 | ppl 16.6 | wps 85180.5 | ups 11.94 | wpb 7133.4 | bsz 227.5 | num_updates 32889 | lr 0.000246598 | gnorm 0.637 | clip 4.1 | loss_scale 4 | train_wall 156 | gb_free 43.2 | wall 2768\n",
      "2022-08-05 18:11:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 018:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 18:11:25 | INFO | fairseq.trainer | begin training epoch 18\n",
      "2022-08-05 18:11:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 018: 100%|▉| 1933/1935 [02:40<00:00, 11.65it/s, loss=5.449, nll_loss=4.0472022-08-05 18:14:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 018 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  33%|██▋     | 3/9 [00:00<00:00, 24.30it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  89%|███████ | 8/9 [00:00<00:00, 36.39it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 18:14:06 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.358 | nll_loss 5.009 | ppl 32.2 | wps 153391 | wpb 3341.8 | bsz 129.6 | num_updates 34824 | best_loss 6.341\n",
      "2022-08-05 18:14:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 34824 updates\n",
      "2022-08-05 18:14:06 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint18.pt\n",
      "2022-08-05 18:14:07 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint18.pt\n",
      "2022-08-05 18:14:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint18.pt (epoch 18 @ 34824 updates, score 6.358) (writing took 2.1717889530118555 seconds)\n",
      "2022-08-05 18:14:08 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
      "2022-08-05 18:14:08 | INFO | train | epoch 018 | loss 5.429 | nll_loss 4.022 | ppl 16.25 | wps 84550.9 | ups 11.85 | wpb 7133.4 | bsz 227.5 | num_updates 34824 | lr 0.000239649 | gnorm 0.645 | clip 4.5 | loss_scale 4 | train_wall 157 | gb_free 43.1 | wall 2931\n",
      "2022-08-05 18:14:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 019:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 18:14:08 | INFO | fairseq.trainer | begin training epoch 19\n",
      "2022-08-05 18:14:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 019: 100%|▉| 1933/1935 [02:38<00:00, 12.29it/s, loss=5.477, nll_loss=4.0782022-08-05 18:16:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 019 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  44%|███▌    | 4/9 [00:00<00:00, 33.56it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset: 100%|████████| 9/9 [00:00<00:00, 41.73it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 18:16:47 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.345 | nll_loss 4.983 | ppl 31.63 | wps 160938 | wpb 3341.8 | bsz 129.6 | num_updates 36759 | best_loss 6.341\n",
      "2022-08-05 18:16:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 36759 updates\n",
      "2022-08-05 18:16:47 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint19.pt\n",
      "2022-08-05 18:16:48 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint19.pt\n",
      "2022-08-05 18:16:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint19.pt (epoch 19 @ 36759 updates, score 6.345) (writing took 2.163998391944915 seconds)\n",
      "2022-08-05 18:16:49 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
      "2022-08-05 18:16:49 | INFO | train | epoch 019 | loss 5.402 | nll_loss 3.991 | ppl 15.9 | wps 85430 | ups 11.98 | wpb 7133.4 | bsz 227.5 | num_updates 36759 | lr 0.000233256 | gnorm 0.645 | clip 4.4 | loss_scale 8 | train_wall 155 | gb_free 43.2 | wall 3093\n",
      "2022-08-05 18:16:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1935\n",
      "epoch 020:   0%|                                       | 0/1935 [00:00<?, ?it/s]2022-08-05 18:16:49 | INFO | fairseq.trainer | begin training epoch 20\n",
      "2022-08-05 18:16:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 020:  95%|▉| 1843/1935 [02:31<00:07, 11.92it/s, loss=5.457, nll_loss=4.0552022-08-05 18:19:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
      "epoch 020: 100%|▉| 1933/1935 [02:38<00:00, 12.06it/s, loss=5.457, nll_loss=4.0552022-08-05 18:19:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 020 | valid on 'valid' subset:   0%|                | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  44%|███▌    | 4/9 [00:00<00:00, 31.44it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset: 100%|████████| 9/9 [00:00<00:00, 40.88it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-05 18:19:29 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.368 | nll_loss 5.021 | ppl 32.47 | wps 155270 | wpb 3341.8 | bsz 129.6 | num_updates 38693 | best_loss 6.341\n",
      "2022-08-05 18:19:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 38693 updates\n",
      "2022-08-05 18:19:29 | INFO | fairseq.trainer | Saving checkpoint to /workspace/data/ch10/95_trained/checkpoint20.pt\n",
      "2022-08-05 18:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/data/ch10/95_trained/checkpoint20.pt\n",
      "2022-08-05 18:19:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ../data/ch10/95_trained/checkpoint20.pt (epoch 20 @ 38693 updates, score 6.368) (writing took 2.1799553600139916 seconds)\n",
      "2022-08-05 18:19:31 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
      "2022-08-05 18:19:31 | INFO | train | epoch 020 | loss 5.379 | nll_loss 3.964 | ppl 15.61 | wps 85427.6 | ups 11.97 | wpb 7134.1 | bsz 226.6 | num_updates 38693 | lr 0.000227352 | gnorm 0.65 | clip 4.2 | loss_scale 4 | train_wall 155 | gb_free 43.6 | wall 3255\n",
      "2022-08-05 18:19:31 | INFO | fairseq_cli.train | done training in 3254.4 seconds\n"
     ]
    }
   ],
   "source": [
    "# モデルの訓練\n",
    "!fairseq-train ../data/ch10/95_preprocessed \\\n",
    "    --save-dir ../data/ch10/95_trained \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --lr-scheduler inverse_sqrt --warmup-updates 2000 --warmup-init-lr 1e-7 \\\n",
    "    --lr 1e-3 \\\n",
    "    --dropout 0.2 \\\n",
    "    --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --max-tokens 8000 \\\n",
    "    --max-epoch 20 \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 翻訳の実行 (92相当)\n",
    "1. 日本語文のトークン化\n",
    "2. 翻訳の実行\n",
    "3. 結果の出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与えられた文のトークン化\n",
    "\n",
    "def ja_tokenizer(text, sp_ja):\n",
    "    token = ' '.join(sp_ja.EncodeAsPieces(text)) + '\\n'\n",
    "    return token\n",
    "\n",
    "# 95_input.ja に保存\n",
    "with open('../data/ch10/95_input.ja', 'w') as f:\n",
    "    f.writelines(ja_tokenizer('道元（どうげん）は、鎌倉時代初期の禅僧。', sp_ja))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-05 23:02:31 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 23:02:32 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../data/ch10/95_trained/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 23:02:32 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 23:02:32 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 23:02:32 | INFO | fairseq_cli.interactive | loading model(s) from ../data/ch10/95_trained/checkpoint_best.pt\n",
      "2022-08-05 23:02:35 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-05 23:02:35 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-05 23:02:36 | INFO | fairseq_cli.interactive | Total time: 3.630 seconds; translation time: 0.709\n"
     ]
    }
   ],
   "source": [
    "# 翻訳を実行し, 95_output.en に保存\n",
    "!fairseq-interactive --path ../data/ch10/95_trained/checkpoint_best.pt ../data/ch10/95_preprocessed < ../data/ch10/95_input.ja  | grep '^H' | cut -f3 > ../data/ch10/95_output.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁Zen ▁Buddhism ▁was ▁introduced ▁into ▁Japan ▁in ▁the ▁middle ▁of ▁the ▁Kamakura ▁period .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 結果の出力\n",
    "with open('../data/ch10/95_output.en', encoding='utf-8') as f:\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEUスコアの計測 (93相当)\n",
    "1. devデータの翻訳\n",
    "2. BLEUスコアの計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-05 23:03:03 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 23:03:05 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../data/ch10/95_trained/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 23:03:05 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 23:03:05 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 23:03:05 | INFO | fairseq_cli.interactive | loading model(s) from ../data/ch10/95_trained/checkpoint_best.pt\n",
      "2022-08-05 23:03:09 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-05 23:03:09 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-05 23:07:03 | INFO | fairseq_cli.interactive | Total time: 237.771 seconds; translation time: 229.091\n"
     ]
    }
   ],
   "source": [
    "!fairseq-interactive --path ../data/ch10/95_trained/checkpoint_best.pt ../data/ch10/95_preprocessed < ../data/ch10/95_dev_tokens.ja | grep '^H' | cut -f3 > ../data/ch10/95_dev_transformed.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-05 23:07:04 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "Namespace(ignore_case=False, order=4, ref='../data/ch10/95_dev_tokens.en', sacrebleu=False, sentence_bleu=False, sys='../data/ch10/95_dev_transformed.en')\n",
      "BLEU4 = 3.64, 22.7/5.4/1.9/0.8 (BP=1.000, ratio=1.118, syslen=32325, reflen=28910)\n"
     ]
    }
   ],
   "source": [
    "!fairseq-score --sys '../data/ch10/95_dev_transformed.en' --ref '../data/ch10/95_dev_tokens.en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEUスコアの計測 (94相当)\n",
    "1. devデータの翻訳\n",
    "2. BLEUスコアの計測\n",
    "3. 結果のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-05 23:07:06 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 23:07:08 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../data/ch10/95_trained/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 23:07:08 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 23:07:08 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 23:07:08 | INFO | fairseq_cli.interactive | loading model(s) from ../data/ch10/95_trained/checkpoint_best.pt\n",
      "2022-08-05 23:07:11 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-05 23:07:11 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-05 23:10:57 | INFO | fairseq_cli.interactive | Total time: 228.453 seconds; translation time: 221.086\n",
      "2022-08-05 23:10:58 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 23:11:00 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../data/ch10/95_trained/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 10, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 23:11:00 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 23:11:00 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 23:11:00 | INFO | fairseq_cli.interactive | loading model(s) from ../data/ch10/95_trained/checkpoint_best.pt\n",
      "2022-08-05 23:11:02 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-05 23:11:02 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-05 23:15:36 | INFO | fairseq_cli.interactive | Total time: 276.373 seconds; translation time: 269.231\n",
      "2022-08-05 23:15:37 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 23:15:39 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../data/ch10/95_trained/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 20, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 23:15:39 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 23:15:39 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 23:15:39 | INFO | fairseq_cli.interactive | loading model(s) from ../data/ch10/95_trained/checkpoint_best.pt\n",
      "2022-08-05 23:15:42 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-05 23:15:42 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-05 23:20:15 | INFO | fairseq_cli.interactive | Total time: 276.051 seconds; translation time: 268.668\n",
      "2022-08-05 23:20:17 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 23:20:18 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../data/ch10/95_trained/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 30, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 23:20:18 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 23:20:18 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 23:20:18 | INFO | fairseq_cli.interactive | loading model(s) from ../data/ch10/95_trained/checkpoint_best.pt\n",
      "2022-08-05 23:20:21 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-05 23:20:21 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-05 23:25:06 | INFO | fairseq_cli.interactive | Total time: 288.161 seconds; translation time: 280.730\n",
      "2022-08-05 23:25:08 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 23:25:10 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../data/ch10/95_trained/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 40, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 23:25:10 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 23:25:10 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 23:25:10 | INFO | fairseq_cli.interactive | loading model(s) from ../data/ch10/95_trained/checkpoint_best.pt\n",
      "2022-08-05 23:25:13 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-05 23:25:13 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-05 23:30:08 | INFO | fairseq_cli.interactive | Total time: 298.611 seconds; translation time: 290.124\n",
      "2022-08-05 23:30:10 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 23:30:11 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../data/ch10/95_trained/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 50, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 23:30:11 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 23:30:11 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 23:30:11 | INFO | fairseq_cli.interactive | loading model(s) from ../data/ch10/95_trained/checkpoint_best.pt\n",
      "2022-08-05 23:30:14 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-05 23:30:14 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-05 23:35:04 | INFO | fairseq_cli.interactive | Total time: 292.818 seconds; translation time: 285.574\n",
      "2022-08-05 23:35:06 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 23:35:07 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../data/ch10/95_trained/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 60, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 23:35:07 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 23:35:07 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 23:35:07 | INFO | fairseq_cli.interactive | loading model(s) from ../data/ch10/95_trained/checkpoint_best.pt\n",
      "2022-08-05 23:35:10 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-05 23:35:10 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-05 23:40:02 | INFO | fairseq_cli.interactive | Total time: 294.440 seconds; translation time: 286.666\n",
      "2022-08-05 23:40:03 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 23:40:05 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../data/ch10/95_trained/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 70, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 23:40:05 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 23:40:05 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 23:40:05 | INFO | fairseq_cli.interactive | loading model(s) from ../data/ch10/95_trained/checkpoint_best.pt\n",
      "2022-08-05 23:40:08 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-05 23:40:08 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-05 23:45:12 | INFO | fairseq_cli.interactive | Total time: 306.742 seconds; translation time: 298.598\n",
      "2022-08-05 23:45:13 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 23:45:15 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../data/ch10/95_trained/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 80, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 23:45:15 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 23:45:15 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 23:45:15 | INFO | fairseq_cli.interactive | loading model(s) from ../data/ch10/95_trained/checkpoint_best.pt\n",
      "2022-08-05 23:45:18 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-05 23:45:18 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-05 23:50:17 | INFO | fairseq_cli.interactive | Total time: 302.216 seconds; translation time: 294.838\n",
      "2022-08-05 23:50:19 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 23:50:20 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../data/ch10/95_trained/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 90, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 23:50:20 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 23:50:20 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 23:50:20 | INFO | fairseq_cli.interactive | loading model(s) from ../data/ch10/95_trained/checkpoint_best.pt\n",
      "2022-08-05 23:50:24 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-05 23:50:24 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-05 23:55:34 | INFO | fairseq_cli.interactive | Total time: 314.217 seconds; translation time: 305.920\n",
      "2022-08-05 23:55:36 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-05 23:55:38 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '../data/ch10/95_trained/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 100, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': '../data/ch10/95_preprocessed', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-08-05 23:55:38 | INFO | fairseq.tasks.translation | [ja] dictionary: 16632 types\n",
      "2022-08-05 23:55:38 | INFO | fairseq.tasks.translation | [en] dictionary: 16784 types\n",
      "2022-08-05 23:55:38 | INFO | fairseq_cli.interactive | loading model(s) from ../data/ch10/95_trained/checkpoint_best.pt\n",
      "2022-08-05 23:55:40 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-05 23:55:40 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 00:00:58 | INFO | fairseq_cli.interactive | Total time: 320.761 seconds; translation time: 313.180\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export CUDA_VISIBLE_DEVICES=1\n",
    "for N in 1 10 20 30 40 50 60 70 80 90 100\n",
    "do\n",
    "fairseq-interactive --path ../data/ch10/95_trained/checkpoint_best.pt --beam $N ../data/ch10/95_preprocessed < ../data/ch10/95_dev_tokens.ja | grep '^H' | cut -f3 > ../data/ch10/95_dev_transformed_$N.en\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-06 00:01:00 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-06 00:01:02 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-06 00:01:03 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-06 00:01:05 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-06 00:01:07 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-06 00:01:09 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-06 00:01:11 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-06 00:01:12 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-06 00:01:14 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-06 00:01:16 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-06 00:01:18 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export CUDA_VISIBLE_DEVICES=1\n",
    "for N in 1 10 20 30 40 50 60 70 80 90 100\n",
    "do\n",
    "fairseq-score --sys ../data/ch10/95_dev_transformed_$N.en --ref ../data/ch10/95_dev_tokens.en > ../data/ch10/95_bleu_$N.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "beams = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "bleus = []\n",
    "for i in beams:\n",
    "    with open('../data/ch10/95_bleu_' + str(i) + '.txt') as f:\n",
    "        x = f.readlines()[1]\n",
    "        bleus.append(float(re.search(r'(BLEU4 = )(\\d*\\.\\d*)(,)', x)[2]))\n",
    "\n",
    "plt.plot(beams, bleus, marker=\"o\")\n",
    "plt.xlabel('beam width')\n",
    "plt.ylabel('bleu score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
