{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 97. ハイパー・パラメータの調整\n",
    "ニューラルネットワークのモデルや，そのハイパーパラメータを変更しつつ，開発データにおけるBLEUスコアが最大となるモデルとハイパーパラメータを求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU prepare\n",
    "1. 使用可能GPUの確認\n",
    "2. GPUの指定\n",
    "3. PyTorchで利用できるGPU数の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  6 07:11:34 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:01:00.0 Off |                  Off |\n",
      "| 30%   31C    P8    24W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    On   | 00000000:25:00.0 Off |                  Off |\n",
      "| 37%   66C    P2   262W / 300W |  28586MiB / 48685MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    On   | 00000000:41:00.0 Off |                  Off |\n",
      "| 30%   27C    P8    30W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    On   | 00000000:61:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    19W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000    On   | 00000000:81:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    17W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000    On   | 00000000:A1:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    18W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000    On   | 00000000:C1:00.0 Off |                  Off |\n",
      "| 30%   28C    P8    16W / 300W |      8MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA RTX A6000    On   | 00000000:E1:00.0 Off |                  Off |\n",
      "| 42%   68C    P2   268W / 300W |  12189MiB / 48685MiB |     94%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# 使用可能GPUの確認\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUの指定\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' #0番を使用するとき"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 確認\n",
    "import torch\n",
    "print(torch.cuda.device_count()) #Pytorchで使用できるGPU数を取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータ探索\n",
    "0. layers 6 ffn-embed-dim 2048 dropout 0.2 (base 91_trained) loss 4.812\n",
    "1. layers 6 ffn-embed-dim 1024 normalize-before dropout 0.2  loss 3.721\n",
    "2. layers 7 ffn-embed-dim 1024 normalize-before dropout 0.2  loss 3.7\n",
    "3. layers 8 ffn-embed-dim 1024 normalize-before dropout 0.2  loss 3.694\n",
    "4. layers 5 ffn-embed-dim 1024 normalize-before dropout 0.2  loss 3.785\n",
    "5. layers 8 ffn-embed-dim 1024 normalize-before dropout 0.3  loss 3.946\n",
    "6. layers 8 ffn-embed-dim 1024 normalize-before dropout 0.1  loss 3.343\n",
    "7. layers 8 ffn-embed-dim 2048 normalize-before dropout 0.1  loss 3.235"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trained_1: layers 6, ffn-embed-dim 1024, normalize-before\n",
    "best: epoch20  loss 3.721 | nll_loss 1.862 | ppl 3.63 | wps 62663.4 | ups 9.2 | wpb 6809.5 | bsz 243.4 | num_updates 36176 | lr 0.000235128 | gnorm 0.387 | clip 0.3 | loss_scale 32 | train_wall 190 | gb_free 39.9 | wall 3912  3911.8 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffn-embed-dimを1024に変更\n",
    "# normalize-beforeを追加\n",
    "!fairseq-train ../data/ch10/91_preprocessed \\\n",
    "    --save-dir ../data/ch10/97_trained_1 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --encoder-layers 6 --decoder-layers 6 \\\n",
    "    --encoder-embed-dim 512 --decoder-embed-dim 512 \\\n",
    "    --encoder-ffn-embed-dim 1024 --decoder-ffn-embed-dim 1024 \\\n",
    "    --encoder-attention-heads 8 --decoder-attention-heads 8 \\\n",
    "    --encoder-normalize-before --decoder-normalize-before \\\n",
    "    --lr-scheduler inverse_sqrt --warmup-updates 2000 --warmup-init-lr 1e-7 \\\n",
    "    --lr 1e-3 \\\n",
    "    --dropout 0.2 \\\n",
    "    --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --max-tokens 8000 \\\n",
    "    --max-epoch 20 \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trained_2: layers 7, ffn-embed-dim 1024, normalize-before\n",
    "best: epoch20  loss 3.7 | nll_loss 1.838 | ppl 3.58 | wps 57178.1 | ups 8.4 | wpb 6809.5 | bsz 243.4 | num_updates 36175 | lr 0.000235131 | gnorm 0.381 | clip 0.2 | loss_scale 8 | train_wall 208 | gb_free 39.6 | wall 4312  4312.3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの層数を7に変更\n",
    "# ffn-embed-dimを1024に変更\n",
    "# normalize-beforeを追加\n",
    "!fairseq-train ../data/ch10/91_preprocessed \\\n",
    "    --save-dir ../data/ch10/97_trained_2 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --encoder-layers 7 --decoder-layers 7 \\\n",
    "    --encoder-embed-dim 512 --decoder-embed-dim 512 \\\n",
    "    --encoder-ffn-embed-dim 1024 --decoder-ffn-embed-dim 1024 \\\n",
    "    --encoder-attention-heads 8 --decoder-attention-heads 8 \\\n",
    "    --encoder-normalize-before --decoder-normalize-before \\\n",
    "    --lr-scheduler inverse_sqrt --warmup-updates 2000 --warmup-init-lr 1e-7 \\\n",
    "    --lr 1e-3 \\\n",
    "    --dropout 0.2 \\\n",
    "    --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --max-tokens 8000 \\\n",
    "    --max-epoch 20 \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trained_3: layers 8, ffn-embed-dim 1024, normalize-before\n",
    "best: epoch20  loss 3.694 | nll_loss 1.832 | ppl 3.56 | wps 53274.9 | ups 7.82 | wpb 6809.5 | bsz 243.4 | num_updates 36176 | lr 0.000235128 | gnorm 0.376 | clip 0.2 | loss_scale 32 | train_wall 223 | gb_free 39.2 | wall 4686  4685.5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの層数を8に変更\n",
    "# ffn-embed-dimを1024に変更\n",
    "# normalize-beforeを追加\n",
    "!fairseq-train ../data/ch10/91_preprocessed \\\n",
    "    --save-dir ../data/ch10/97_trained_3 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --encoder-layers 8 --decoder-layers 8 \\\n",
    "    --encoder-embed-dim 512 --decoder-embed-dim 512 \\\n",
    "    --encoder-ffn-embed-dim 1024 --decoder-ffn-embed-dim 1024 \\\n",
    "    --encoder-attention-heads 8 --decoder-attention-heads 8 \\\n",
    "    --encoder-normalize-before --decoder-normalize-before \\\n",
    "    --lr-scheduler inverse_sqrt --warmup-updates 2000 --warmup-init-lr 1e-7 \\\n",
    "    --lr 1e-3 \\\n",
    "    --dropout 0.2 \\\n",
    "    --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --max-tokens 8000 \\\n",
    "    --max-epoch 20 \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trained_4: layers 5, ffn-embed-dim 1024, normalize-before\n",
    "best: epoch20  loss 3.785 | nll_loss 1.934 | ppl 3.82 | wps 68689.2 | ups 10.09 | wpb 6809.5 | bsz 243.4 | num_updates 36176 | lr 0.000235128 | gnorm 0.396 | clip 0.3 | loss_scale 32 | train_wall 173 | gb_free 40.3 | wall 3616  3615.7 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの層数を5に変更\n",
    "# ffn-embed-dimを1024に変更\n",
    "# normalize-beforeを追加\n",
    "!fairseq-train ../data/ch10/91_preprocessed \\\n",
    "    --save-dir ../data/ch10/97_trained_4 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --encoder-layers 5 --decoder-layers 5 \\\n",
    "    --encoder-embed-dim 512 --decoder-embed-dim 512 \\\n",
    "    --encoder-ffn-embed-dim 1024 --decoder-ffn-embed-dim 1024 \\\n",
    "    --encoder-attention-heads 8 --decoder-attention-heads 8 \\\n",
    "    --encoder-normalize-before --decoder-normalize-before \\\n",
    "    --lr-scheduler inverse_sqrt --warmup-updates 2000 --warmup-init-lr 1e-7 \\\n",
    "    --lr 1e-3 \\\n",
    "    --dropout 0.2 \\\n",
    "    --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --max-tokens 8000 \\\n",
    "    --max-epoch 20 \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trained_5: layers 8, ffn-embed-dim 1024, normalize-before, dropout 0.3\n",
    "best: loss 3.946 | nll_loss 2.121 | ppl 4.35 | wps 45994 | ups 6.75 | wpb 6809.5 | bsz 243.4 | num_updates 36176 | lr 0.000235128 | gnorm 0.36 | clip 0.3 | loss_scale 32 | train_wall 259 | gb_free 39.2 | wall 4937  4936.4 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドロップアウト率を0.3に変更\n",
    "# モデルの層数を8に変更\n",
    "# ffn-embed-dimを1024に変更\n",
    "# normalize-beforeを追加\n",
    "!fairseq-train ../data/ch10/91_preprocessed \\\n",
    "    --save-dir ../data/ch10/97_trained_5 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --encoder-layers 8 --decoder-layers 8 \\\n",
    "    --encoder-embed-dim 512 --decoder-embed-dim 512 \\\n",
    "    --encoder-ffn-embed-dim 1024 --decoder-ffn-embed-dim 1024 \\\n",
    "    --encoder-attention-heads 8 --decoder-attention-heads 8 \\\n",
    "    --encoder-normalize-before --decoder-normalize-before \\\n",
    "    --lr-scheduler inverse_sqrt --warmup-updates 2000 --warmup-init-lr 1e-7 \\\n",
    "    --lr 1e-3 \\\n",
    "    --dropout 0.3 \\\n",
    "    --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --max-tokens 8000 \\\n",
    "    --max-epoch 20 \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trained_6: layers 8, ffn-embed-dim 1024, normalize-before, dropout 0.1\n",
    "best: epoch20  loss 3.343 | nll_loss 1.424 | ppl 2.68 | wps 53495.8 | ups 7.86 | wpb 6809.5 | bsz 243.4 | num_updates 36175 | lr 0.000235131 | gnorm 0.421 | clip 0.1 | loss_scale 16 | train_wall 222 | gb_free 39.2 | wall 4607  4606.7 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドロップアウト率を0.1に変更\n",
    "# モデルの層数を8に変更\n",
    "# ffn-embed-dimを1024に変更\n",
    "# normalize-beforeを追加\n",
    "!fairseq-train ../data/ch10/91_preprocessed \\\n",
    "    --save-dir ../data/ch10/97_trained_6 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --encoder-layers 8 --decoder-layers 8 \\\n",
    "    --encoder-embed-dim 512 --decoder-embed-dim 512 \\\n",
    "    --encoder-ffn-embed-dim 1024 --decoder-ffn-embed-dim 1024 \\\n",
    "    --encoder-attention-heads 8 --decoder-attention-heads 8 \\\n",
    "    --encoder-normalize-before --decoder-normalize-before \\\n",
    "    --lr-scheduler inverse_sqrt --warmup-updates 2000 --warmup-init-lr 1e-7 \\\n",
    "    --lr 1e-3 \\\n",
    "    --dropout 0.1 \\\n",
    "    --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --max-tokens 8000 \\\n",
    "    --max-epoch 20 \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trained_7: layers 8, ffn-embed-dim 2048, normalize-before, dropout 0.1\n",
    "best: epoch20  loss 3.235 | nll_loss 1.301 | ppl 2.46 | wps 48725.4 | ups 7.16 | wpb 6809.5 | bsz 243.4 | num_updates 36176 | lr 0.000235128 | gnorm 0.421 | clip 0.1 | loss_scale 32 | train_wall 243 | gb_free 38.6 | wall 5780  5779.6 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドロップアウト率を0.1に変更\n",
    "# モデルの層数を8に変更\n",
    "# normalize-beforeを追加\n",
    "!fairseq-train ../data/ch10/91_preprocessed \\\n",
    "    --save-dir ../data/ch10/97_trained_7 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --encoder-layers 8 --decoder-layers 8 \\\n",
    "    --encoder-embed-dim 512 --decoder-embed-dim 512 \\\n",
    "    --encoder-ffn-embed-dim 2048 --decoder-ffn-embed-dim 2048 \\\n",
    "    --encoder-attention-heads 8 --decoder-attention-heads 8 \\\n",
    "    --encoder-normalize-before --decoder-normalize-before \\\n",
    "    --lr-scheduler inverse_sqrt --warmup-updates 2000 --warmup-init-lr 1e-7 \\\n",
    "    --lr 1e-3 \\\n",
    "    --dropout 0.1 \\\n",
    "    --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --max-tokens 8000 \\\n",
    "    --max-epoch 20 \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEUスコアの比較\n",
    "1. devデータの翻訳\n",
    "2. BLEUスコアの計測\n",
    "3. BLEUスコアの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export CUDA_VISIBLE_DEVICES=0\n",
    "for N in 1 2 3 4 5 6 7\n",
    "do\n",
    "fairseq-interactive --path ../data/ch10/97_trained_$N/checkpoint_best.pt --beam 20 ../data/ch10/91_preprocessed < ../data/ch10/90_dev_tokens.ja | grep '^H' | cut -f3 > ../data/ch10/97_dev_transformed_$N.en\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export CUDA_VISIBLE_DEVICES=1\n",
    "for N in 1 2 3 4 5 6 7\n",
    "do\n",
    "fairseq-score --sys ../data/ch10/97_dev_transformed_$N.en --ref ../data/ch10/90_dev_tokens.en > ../data/ch10/97_bleu_$N.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0: dropout 0.2, encoder(decoder)-layers 6 ffn-embed-dim 2048                  => loss is 4.812  dev bleu is 6.05\n",
      "model_1: dropout 0.2, encoder(decoder)-layers 6 ffn-embed-dim 1024 normalize-before => loss is 3.721  dev bleu is 20.61\n",
      "model_2: dropout 0.2, encoder(decoder)-layers 7 ffn-embed-dim 1024 normalize-before => loss is 3.700  dev bleu is 21.28\n",
      "model_3: dropout 0.2, encoder(decoder)-layers 8 ffn-embed-dim 1024 normalize-before => loss is 3.694  dev bleu is 20.13\n",
      "model_4: dropout 0.2, encoder(decoder)-layers 5 ffn-embed-dim 1024 normalize-before => loss is 3.785  dev bleu is 20.39\n",
      "model_5: dropout 0.3, encoder(decoder)-layers 8 ffn-embed-dim 1024 normalize-before => loss is 3.946  dev bleu is 20.36\n",
      "model_6: dropout 0.1, encoder(decoder)-layers 8 ffn-embed-dim 1024 normalize-before => loss is 3.343  dev bleu is 19.42\n",
      "model_7: dropout 0.1, encoder(decoder)-layers 8 ffn-embed-dim 2048 normalize-before => loss is 3.235  dev bleu is 19.17\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "bleus = []\n",
    "\n",
    "with open('../data/ch10/94_bleu_20.txt') as f:\n",
    "    x = f.readlines()[1]\n",
    "    bleus.append(float(re.search(r'(BLEU4 = )(\\d*\\.\\d*)(,)', x)[2]))\n",
    "\n",
    "for i in range(1,8):\n",
    "    with open('../data/ch10/97_bleu_' + str(i) + '.txt') as f:\n",
    "        x = f.readlines()[1]\n",
    "        bleus.append(float(re.search(r\"(BLEU4 = )(\\d*\\.\\d*)(,)\", x)[2]))\n",
    "\n",
    "print(f'model_0: dropout 0.2, encoder(decoder)-layers 6 ffn-embed-dim 2048                  => loss is 4.812  dev bleu is {bleus[0]}')\n",
    "print(f'model_1: dropout 0.2, encoder(decoder)-layers 6 ffn-embed-dim 1024 normalize-before => loss is 3.721  dev bleu is {bleus[1]}')\n",
    "print(f'model_2: dropout 0.2, encoder(decoder)-layers 7 ffn-embed-dim 1024 normalize-before => loss is 3.700  dev bleu is {bleus[2]}')\n",
    "print(f'model_3: dropout 0.2, encoder(decoder)-layers 8 ffn-embed-dim 1024 normalize-before => loss is 3.694  dev bleu is {bleus[3]}')\n",
    "print(f'model_4: dropout 0.2, encoder(decoder)-layers 5 ffn-embed-dim 1024 normalize-before => loss is 3.785  dev bleu is {bleus[4]}')\n",
    "print(f'model_5: dropout 0.3, encoder(decoder)-layers 8 ffn-embed-dim 1024 normalize-before => loss is 3.946  dev bleu is {bleus[5]}')\n",
    "print(f'model_6: dropout 0.1, encoder(decoder)-layers 8 ffn-embed-dim 1024 normalize-before => loss is 3.343  dev bleu is {bleus[6]}')\n",
    "print(f'model_7: dropout 0.1, encoder(decoder)-layers 8 ffn-embed-dim 2048 normalize-before => loss is 3.235  dev bleu is {bleus[7]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
