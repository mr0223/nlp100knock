{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 90. データの準備\n",
    "機械翻訳のデータセットをダウンロードせよ．訓練データ，開発データ，評価データを整形し，必要に応じてトークン化などの前処理を行うこと．ただし，この段階ではトークンの単位として形態素（日本語）および単語（英語）を採用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トークン化\n",
    "1. 日本語のトークン化関数 (形態素)\n",
    "2. 英語のトークン化関数 (単語)\n",
    "3. テキストのトークン化\n",
    "4. データの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeCab ipadic NEologdによる形態素分割\n",
    "import MeCab\n",
    "\n",
    "wakati = MeCab.Tagger(\"/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd -Owakati\")\n",
    "\n",
    "def ja_tokenizer(fname):\n",
    "    '''\n",
    "    input :fname\n",
    "    output:list(token)\n",
    "    '''\n",
    "    with open(fname, encoding='utf-8') as f1:\n",
    "        lines = f1.readlines()\n",
    "    \n",
    "    tokens = [wakati.parse(line) for line in lines]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mosestokenizerによる単語分割\n",
    "from mosestokenizer import *\n",
    "\n",
    "tokenizer = MosesTokenizer('en')\n",
    "\n",
    "def en_tokenizer(fname):\n",
    "    '''\n",
    "    input :fname\n",
    "    output:list(token)\n",
    "    '''\n",
    "    with open(fname, encoding='utf-8') as f2:\n",
    "        lines = f2.readlines()\n",
    "    \n",
    "    tokens = [' '.join(tokenizer(line)) + '\\n' for line in lines]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ja_tokenizerによるトークン化\n",
    "train_ja_tokens = ja_tokenizer('../data/ch10/kftt-data-1.0/data/orig/kyoto-train.ja')\n",
    "dev_ja_tokens   = ja_tokenizer('../data/ch10/kftt-data-1.0/data/orig/kyoto-dev.ja')\n",
    "test_ja_tokens  = ja_tokenizer('../data/ch10/kftt-data-1.0/data/orig/kyoto-test.ja')\n",
    "\n",
    "# en_tokenizerによるトークン化\n",
    "train_en_tokens = en_tokenizer('../data/ch10/kftt-data-1.0/data/orig/kyoto-train.en')\n",
    "dev_en_tokens   = en_tokenizer('../data/ch10/kftt-data-1.0/data/orig/kyoto-dev.en')\n",
    "test_en_tokens  = en_tokenizer('../data/ch10/kftt-data-1.0/data/orig/kyoto-test.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの保存\n",
    "def save_token(token, fname):\n",
    "    with open(fname, 'w') as f:\n",
    "        f.writelines(token)\n",
    "\n",
    "save_token(train_ja_tokens, '../data/ch10/90_train_tokens.ja')\n",
    "save_token(dev_ja_tokens, '../data/ch10/90_dev_tokens.ja')\n",
    "save_token(test_ja_tokens, '../data/ch10/90_test_tokens.ja')\n",
    "save_token(train_en_tokens, '../data/ch10/90_train_tokens.en')\n",
    "save_token(dev_en_tokens, '../data/ch10/90_dev_tokens.en')\n",
    "save_token(test_en_tokens, '../data/ch10/90_test_tokens.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
