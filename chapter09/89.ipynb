{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8587b66",
   "metadata": {},
   "source": [
    "## GPU prepare\n",
    "1. 使用可能GPUの確認\n",
    "2. GPUの指定\n",
    "3. PyTorchで利用できるGPU数の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b8a87e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug  1 06:28:02 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:01:00.0 Off |                  Off |\n",
      "| 33%   61C    P2   158W / 300W |   1445MiB / 48685MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    On   | 00000000:25:00.0 Off |                  Off |\n",
      "| 30%   27C    P8    14W / 300W |      5MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    On   | 00000000:41:00.0 Off |                  Off |\n",
      "| 30%   28C    P8    32W / 300W |      5MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    On   | 00000000:61:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    19W / 300W |      5MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000    On   | 00000000:81:00.0 Off |                  Off |\n",
      "| 30%   27C    P8    18W / 300W |      5MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000    On   | 00000000:A1:00.0 Off |                  Off |\n",
      "| 30%   25C    P8    18W / 300W |      5MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000    On   | 00000000:C1:00.0 Off |                  Off |\n",
      "| 30%   27C    P8    15W / 300W |      5MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA RTX A6000    On   | 00000000:E1:00.0 Off |                  Off |\n",
      "| 30%   26C    P8    14W / 300W |      5MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# 使用可能GPUの確認\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772a2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUの指定\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' #0番を使用するとき"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dafe4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 確認\n",
    "import torch\n",
    "print(torch.cuda.device_count()) #Pytorchで使用できるGPU数を取得"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f05cde",
   "metadata": {},
   "source": [
    "## prepare\n",
    "1. ~~語彙数の取得~~\n",
    "2. ~~学習データの用意(ラベル)~~\n",
    "3. ~~学習データの用意(特徴量)~~\n",
    "4. 乱数の種を固定\n",
    "5. 学習データの用意(テキスト)\n",
    "6. 学習データの用意(ラベル)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "713279bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練・検証・評価データの用意\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 乱数シードの固定\n",
    "import random\n",
    "\n",
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08802907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "def read_text(fname):\n",
    "    '''\n",
    "    input :fname\n",
    "    output:list(text)\n",
    "    '''\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    titles = [line[2:] for line in lines]\n",
    "\n",
    "    return titles\n",
    "\n",
    "# タイトルの保存\n",
    "X_train_text = read_text('../data/ch06/train.txt')\n",
    "X_valid_text = read_text('../data/ch06/valid.txt')\n",
    "X_test_text = read_text('../data/ch06/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7cfbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラベル: ch08の出力を利用\n",
    "Y_train = np.loadtxt('../data/ch08/Y_train.txt')\n",
    "Y_valid = np.loadtxt('../data/ch08/Y_valid.txt')\n",
    "Y_test = np.loadtxt('../data/ch08/Y_test.txt')\n",
    "\n",
    "# pytorch用に変換\n",
    "Y_train_long = torch.tensor(Y_train, dtype=torch.int64)\n",
    "Y_valid_long = torch.tensor(Y_valid, dtype=torch.int64)\n",
    "Y_test_long = torch.tensor(Y_test, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf04fc0",
   "metadata": {},
   "source": [
    "## 89. 事前学習済み言語モデルからの転移学習\n",
    "1. BERTの前処理/データ準備\n",
    "2. 学習済みモデルのロード\n",
    "3. train, validの定義\n",
    "4. fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97756306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Tokenizerを用いた単語分割・ID変換\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 最大単語数の確認\n",
    "X_all_text = X_train_text + X_valid_text + X_test_text\n",
    "max_lens = []\n",
    "for X_text in X_all_text:\n",
    "    token_words = tokenizer.tokenize(X_text)\n",
    "    max_lens.append(len(token_words))\n",
    "maximam_len = max(max_lens) + 2\n",
    "\n",
    "# 単語分割・ID変換, special tokenの追加, 文章の長さの固定, Attention mask arrayの作成\n",
    "def make_dataset(X, Y):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for X_text in X:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                X_text,\n",
    "                add_special_tokens = True,      # special tokenの追加\n",
    "                max_length = maximam_len,       # 文章の長さの固定\n",
    "                padding = 'max_length',         # PADIING: new_style\n",
    "                truncation = True,              # truncation: new_style\n",
    "                return_attention_mask = True,   # Attention maskの作成\n",
    "                return_tensors = 'pt'           # pytorch tensorsで返却\n",
    "            )\n",
    "            \n",
    "        # 単語IDを取得\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # attention maskの取得\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # リストに入ったtensorを縦方向へ結合(dim=0)\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return TensorDataset(input_ids, attention_masks, Y)\n",
    "\n",
    "train_dataset = make_dataset(X_train_text, Y_train_long)\n",
    "valid_dataset = make_dataset(X_valid_text, Y_valid_long)\n",
    "test_dataset = make_dataset(X_test_text, Y_test_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2a3db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みモデルのロード\n",
    "from transformers import BertModel\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = torch.nn.Linear(768, 4)\n",
    "    \n",
    "    def forward(self, ids, mask):\n",
    "        out = self.bert(ids, attention_mask=mask)\n",
    "        out = self.fc(out[1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a3b0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 損失, 正解率の計算\n",
    "def calculate_loss_accuracy(model, criterion, loader, device, batch_size):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct_num = 0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            ids = data[0].to(device)\n",
    "            mask = data[1].to(device)\n",
    "            labels = data[2].to(device)\n",
    "\n",
    "            # 順伝播\n",
    "            outputs = model(ids, mask)\n",
    "            # 損失計算\n",
    "            loss += criterion(outputs, labels).item()\n",
    "\n",
    "            # 正解率計算\n",
    "            predict_labels = torch.max(outputs, 1)[1]\n",
    "            for i in range(batch_size):\n",
    "                if predict_labels[i] == labels[i]:\n",
    "                    correct_num += 1\n",
    "            total_num += len(labels)\n",
    "    \n",
    "    return loss / len(loader), correct_num / total_num\n",
    "\n",
    "# trainの定義\n",
    "def train(model, train_dataset, valid_dataset, batch_size, criterion, optimizer, num_epochs, device):\n",
    "    # dataloaderの作成\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=True, drop_last=True)\n",
    "    \n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # 開始時間の記録\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 訓練モードで実行\n",
    "        model.train()\n",
    "        for data in train_dataloader:\n",
    "            ids = data[0].to(device)\n",
    "            mask = data[1].to(device)\n",
    "            labels = data[2].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            outputs = model(ids, mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            # 勾配クリッピング\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 損失, 正解率の計算\n",
    "        train_loss, train_acc = calculate_loss_accuracy(model, criterion, train_dataloader, device, batch_size)\n",
    "        valid_loss, valid_acc = calculate_loss_accuracy(model, criterion, valid_dataloader, device, len(valid_dataset))\n",
    "        train_accs.append(train_acc)\n",
    "        valid_accs.append(valid_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        # 終了時刻の記録\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f'epoch: {epoch}, train_loss: {train_loss:.4}, train_accuracy: {train_acc:.4f}, valid_loss: {valid_loss:.4f}, valid_accuracy: {valid_acc:.4f}, {(end_time-start_time):.4f}sec')\n",
    "\n",
    "    return {'train_loss': train_losses, 'train_acc': train_accs, 'valid_loss': valid_losses, 'valid_acc': valid_accs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "916553da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 0.1385, train_accuracy: 0.9604, valid_loss: 0.2236, valid_accuracy: 0.9259, 22.0368sec\n",
      "epoch: 1, train_loss: 0.06551, train_accuracy: 0.9805, valid_loss: 0.2031, valid_accuracy: 0.9364, 21.2869sec\n",
      "epoch: 2, train_loss: 0.04653, train_accuracy: 0.9866, valid_loss: 0.2741, valid_accuracy: 0.9274, 21.2034sec\n",
      "epoch: 3, train_loss: 0.02636, train_accuracy: 0.9932, valid_loss: 0.2733, valid_accuracy: 0.9394, 21.1590sec\n"
     ]
    }
   ],
   "source": [
    "# fine-tuning\n",
    "# ハイパーパラメータ\n",
    "batch_size = 32\n",
    "num_epochs = 4\n",
    "lr = 2e-5\n",
    "\n",
    "# モデルの定義\n",
    "fix_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClass().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr)\n",
    "\n",
    "# モデルの学習\n",
    "log = train(model, train_dataset, valid_dataset, batch_size, criterion, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f449ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
