{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9651c31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display second sentence's morphs\n",
      "display first 10 morphs\n",
      "\n",
      "surface:人工, base:人工, pos:名詞, pos1:一般\n",
      "surface:知能, base:知能, pos:名詞, pos1:一般\n",
      "surface:（, base:（, pos:記号, pos1:括弧開\n",
      "surface:じん, base:じん, pos:名詞, pos1:一般\n",
      "surface:こうち, base:こうち, pos:名詞, pos1:一般\n",
      "surface:のう, base:のう, pos:助詞, pos1:終助詞\n",
      "surface:、, base:、, pos:記号, pos1:読点\n",
      "surface:、, base:、, pos:記号, pos1:読点\n",
      "surface:AI, base:*, pos:名詞, pos1:一般\n",
      "surface:〈, base:〈, pos:記号, pos1:括弧開\n"
     ]
    }
   ],
   "source": [
    "# 40. 係り受け解析結果の読み込み（形態素）\n",
    "import re\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos  = pos\n",
    "        self.pos1 = pos1\n",
    "    def print_morph(self):\n",
    "        print(f'surface:{self.surface}, base:{self.base}, pos:{self.pos}, pos1:{self.pos1}')\n",
    "\n",
    "def make_whole_sentences_morphs(file_name):\n",
    "    '''\n",
    "    input:  file name\n",
    "    output: 各文をmorphオブジェクトのリストで表し、全文を返す\n",
    "    '''\n",
    "    with open(file_name, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    whole_sentences_morphs = []   # 各文のMorphオブジェクトリストをすべて集めたもの\n",
    "    sentence_morphs = []          # 1文のMorphオブジェクトリスト\n",
    "    i = 0                         # 何文目か示す変数\n",
    "    while i < len(lines):\n",
    "        # 1文のループを明示(1文はEOSで区切られる)\n",
    "        while lines[i]!='EOS\\n':\n",
    "            # 形態素を含む列の場合、one_sentence_morphsにmorph追加\n",
    "            if re.findall('(.*?)\\t(.*?),(.*?),(.*?),(.*?),(.*?),(.*?),(.*?)[,\\n]', lines[i]):\n",
    "                tmp = re.findall('(.*?)\\t(.*?),(.*?),(.*?),(.*?),(.*?),(.*?),(.*?)[,\\n]', lines[i])\n",
    "                sentence_morphs.append(Morph(surface=tmp[0][0], base=tmp[0][7], pos=tmp[0][1], pos1=tmp[0][2]))\n",
    "            i += 1\n",
    "        \n",
    "        # sentence_morphsが空なら(EOS\\nが二回続いときは)何もしない\n",
    "        # 中身があれば、whole_sentences_morphsに追加し、空にする    \n",
    "        if sentence_morphs:\n",
    "            whole_sentences_morphs.append(sentence_morphs)\n",
    "            sentence_morphs = []\n",
    "        i += 1\n",
    "    \n",
    "    return whole_sentences_morphs\n",
    "\n",
    "whole_sentences_morphs = make_whole_sentences_morphs('data/ch05/ai.ja.txt.parsed')\n",
    "\n",
    "print('display second sentence\\'s morphs')\n",
    "print('display first 10 morphs\\n')\n",
    "\n",
    "for morph in whole_sentences_morphs[1][:10]:\n",
    "    morph.print_morph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee29f837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display second sentence's chunks\n",
      "display first 10 chunks\n",
      "\n",
      " 0 chunk:人工知能 \tdst:17\n",
      " 1 chunk:（じんこうちのう、、 \tdst:17\n",
      " 2 chunk:AI \tdst:3\n",
      " 3 chunk:〈エーアイ〉）とは、 \tdst:17\n",
      " 4 chunk:「『計算 \tdst:5\n",
      " 5 chunk:（）』という \tdst:9\n",
      " 6 chunk:概念と \tdst:9\n",
      " 7 chunk:『コンピュータ \tdst:8\n",
      " 8 chunk:（）』という \tdst:9\n",
      " 9 chunk:道具を \tdst:10\n"
     ]
    }
   ],
   "source": [
    "# 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "# Morph: def in 40\n",
    "import re\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, id, dst):\n",
    "        self.id  = int(id)\n",
    "        self.dst = int(dst)\n",
    "        self.morphs = []\n",
    "        self.srcs = []\n",
    "    def append_morph(self, morph):\n",
    "        self.morphs.append(morph)\n",
    "    def append_src(self, src):\n",
    "        self.srcs.append(int(src))\n",
    "    def fetch_phrase(self):\n",
    "        phrase = ''\n",
    "        for morph in self.morphs:\n",
    "            phrase += morph.surface\n",
    "        return phrase\n",
    "    def print_chunk(self):\n",
    "        print(f'{self.id:2} chunk:{self.fetch_phrase()} \\tdst:{self.dst}')\n",
    "    def erase_symbol(self):\n",
    "        new_morphs = []\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos=='記号':\n",
    "                continue\n",
    "            else:\n",
    "                new_morphs.append(morph)\n",
    "        self.morphs = new_morphs\n",
    "    def is_noun(self):\n",
    "        '''\n",
    "        名詞が存在する場合、Trueを返す(use in 48,49)\n",
    "        '''\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos=='名詞':\n",
    "                return True\n",
    "        return False\n",
    "    def fetch_replaced_phrase(self, replace):\n",
    "        '''\n",
    "        名詞句をreplaceと置換したphraseを返す(use in 49)\n",
    "        '''\n",
    "        replaced_phrase=''\n",
    "        is_noun_phrase = 1\n",
    "        for num, morph in enumerate(self.morphs):\n",
    "            # 名詞句が終わったらreplaceを追加\n",
    "            if (morph.pos!='名詞') & (morph.pos!='接頭詞') & is_noun_phrase:\n",
    "                is_noun_phrase = 0\n",
    "                replaced_phrase += replace\n",
    "            # 名詞句が最後まで続いてる場合もreplaceを追加\n",
    "            if is_noun_phrase & (num+1==len(self.morphs)):\n",
    "                replaced_phrase += replace\n",
    "            # 名詞句でない場合、表層形をそのまま追加\n",
    "            if not is_noun_phrase:\n",
    "                replaced_phrase += morph.surface\n",
    "        return replaced_phrase\n",
    "\n",
    "def make_whole_sentences_chunks(file_name):\n",
    "    '''\n",
    "    input:  ファイル名\n",
    "    output: 1文をchunkオブジェクトのリストで表し、全文のリストを返す(chunk.srcsは[])\n",
    "    '''\n",
    "    with open(file_name, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    whole_sentences_chunks = []   # 各文のChunkオブジェクトリストをすべて集めたもの\n",
    "    sentence_chunks = []          # 1文のChunkオブジェクトリスト\n",
    "    i = 0                         # 何文目か示す変数\n",
    "    while i < len(lines):\n",
    "        # 1文のループを明示(1文はEOSで区切られる)\n",
    "        while lines[i]!='EOS\\n':\n",
    "            # 1文節のループを明示(1文節は*かEOSで区切られる)\n",
    "            if lines[i][0]=='*':\n",
    "                # 文節情報を含む列の場合、chunkを初期化\n",
    "                tmp = re.findall('\\* (\\d+) (.*?)D', lines[i])\n",
    "                chunk = Chunk(id=tmp[0][0], dst=tmp[0][1])\n",
    "                i += 1\n",
    "                while (lines[i][0]!='*') & (lines[i]!='EOS\\n'):\n",
    "                    # 形態素を含む列の場合、chunkにmorph追加\n",
    "                    if re.findall('(.*?)\\t(.*?),(.*?),(.*?),(.*?),(.*?),(.*?),(.*?)[,\\n]', lines[i]):\n",
    "                        tmp = re.findall('(.*?)\\t(.*?),(.*?),(.*?),(.*?),(.*?),(.*?),(.*?)[,\\n]', lines[i])\n",
    "                        chunk.append_morph(Morph(surface=tmp[0][0], base=tmp[0][7], pos=tmp[0][1], pos1=tmp[0][2]))\n",
    "                    i += 1\n",
    "\n",
    "            sentence_chunks.append(chunk)\n",
    "        \n",
    "        # sentence_chunksが空なら(EOS\\nが二回続いときは)何もしない\n",
    "        # 中身があれば、whole_sentences_chunksに追加し、空にする    \n",
    "        if sentence_chunks:\n",
    "            whole_sentences_chunks.append(sentence_chunks)\n",
    "            sentence_chunks = []\n",
    "        i += 1\n",
    "    \n",
    "    return whole_sentences_chunks\n",
    "\n",
    "def organize_srcs(chunk_lists):\n",
    "    '''\n",
    "    input:  全文のchunkリスト\n",
    "    output：なし(chunkリスト内のsrcsを更新)\n",
    "    '''\n",
    "    for chunk_list in chunk_lists:\n",
    "        for num, chunk in enumerate(chunk_list):\n",
    "            dst = chunk.dst\n",
    "            if dst==-1:\n",
    "                continue\n",
    "            chunk_list[dst].append_src(num)\n",
    "\n",
    "def erase_symbols(chunk_lists):\n",
    "    '''\n",
    "    input;  全文のchunkリスト\n",
    "    output: なし(chunkリスト内の記号を消去)\n",
    "    '''\n",
    "    for chunk_list in chunk_lists:\n",
    "        for chunk in chunk_list:\n",
    "            chunk.erase_symbol()\n",
    "\n",
    "whole_sentences_chunks = make_whole_sentences_chunks('data/ch05/ai.ja.txt.parsed')\n",
    "organize_srcs(whole_sentences_chunks)\n",
    "\n",
    "print('display second sentence\\'s chunks')\n",
    "print('display first 10 chunks\\n')\n",
    "for chunk in whole_sentences_chunks[1][:10]:\n",
    "    chunk.print_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e566751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display first 10 extraction results\n",
      "\n",
      "人工知能\t語\n",
      "じんこうちのう\t語\n",
      "AI\tエーアイとは\n",
      "エーアイとは\t語\n",
      "計算\tという\n",
      "という\t道具を\n",
      "概念と\t道具を\n",
      "コンピュータ\tという\n",
      "という\t道具を\n",
      "道具を\t用いて\n"
     ]
    }
   ],
   "source": [
    "# 42. 係り元と係り先の文節の表示\n",
    "# Chunk, make_whole_sentences_chunks, organize_srcs, erase_symbols: def in 41\n",
    "whole_sentences_chunks = make_whole_sentences_chunks('data/ch05/ai.ja.txt.parsed')\n",
    "organize_srcs(whole_sentences_chunks)\n",
    "erase_symbols(whole_sentences_chunks)\n",
    "\n",
    "relation_list = []   # '係り元\\t係り先'のリスト\n",
    "for sentence_chunks in whole_sentences_chunks:\n",
    "    for chunk in sentence_chunks:\n",
    "        if chunk.dst==-1:\n",
    "            continue\n",
    "        relation_list.append(f'{chunk.fetch_phrase()}\\t{sentence_chunks[chunk.dst].fetch_phrase()}')\n",
    "\n",
    "print('display first 10 extraction results\\n')\n",
    "for relation in relation_list[:10]:\n",
    "    print(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f41c88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display first 10 extraction results\n",
      "\n",
      "道具を\t用いて\n",
      "知能を\t研究する\n",
      "一分野を\t指す\n",
      "知的行動を\t代わって\n",
      "人間に\t代わって\n",
      "コンピューターに\t行わせる\n",
      "研究分野とも\tされる\n",
      "解説で\t述べている\n",
      "佐藤理史は\t述べている\n",
      "次のように\t述べている\n"
     ]
    }
   ],
   "source": [
    "# 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "whole_sentences_chunks = make_whole_sentences_chunks('data/ch05/ai.ja.txt.parsed')\n",
    "organize_srcs(whole_sentences_chunks)\n",
    "erase_symbols(whole_sentences_chunks)\n",
    "\n",
    "def judge_contained(chunk, pos):\n",
    "    '''\n",
    "    input:  chunk, pos\n",
    "    output: chunkがposを含む場合True, 含まない場合False\n",
    "    '''\n",
    "    for morph in chunk.morphs:\n",
    "        if morph.pos == pos:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "specified_relation_list = []   # '名詞を含む係り元\\t動詞を含む係り先'のリスト\n",
    "for sentence_chunks in whole_sentences_chunks:\n",
    "    for chunk in sentence_chunks:\n",
    "        if chunk.dst==-1:\n",
    "            continue\n",
    "        # 係り元が名詞を含み、係り先が動詞を含むか確認\n",
    "        if judge_contained(chunk, '名詞') & judge_contained(sentence_chunks[chunk.dst], '動詞'):\n",
    "            specified_relation_list.append(f'{chunk.fetch_phrase()}\\t{sentence_chunks[chunk.dst].fetch_phrase()}')\n",
    "\n",
    "print('display first 10 extraction results\\n')\n",
    "for specified_relation in specified_relation_list[:10]:\n",
    "    print(specified_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f2249f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 3.0.0 (20220226.1711)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"587pt\" height=\"260pt\"\n",
       " viewBox=\"0.00 0.00 587.23 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-256 583.23,-256 583.23,4 -4,4\"/>\n",
       "<!-- 1.人工知能は -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1.人工知能は</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"152.09\" cy=\"-90\" rx=\"73.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"152.09\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.人工知能は</text>\n",
       "</g>\n",
       "<!-- 0.第２次人工知能ブームでの -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>0.第２次人工知能ブームでの</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"152.09\" cy=\"-18\" rx=\"152.17\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"152.09\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0.第２次人工知能ブームでの</text>\n",
       "</g>\n",
       "<!-- 1.人工知能は&#45;&gt;0.第２次人工知能ブームでの -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1.人工知能は&#45;&gt;0.第２次人工知能ブームでの</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M152.09,-71.7C152.09,-63.98 152.09,-54.71 152.09,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"155.59,-46.1 152.09,-36.1 148.59,-46.1 155.59,-46.1\"/>\n",
       "</g>\n",
       "<!-- 3.呼ばれ -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3.呼ばれ</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"317.09\" cy=\"-162\" rx=\"51.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"317.09\" y=\"-158.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">3.呼ばれ</text>\n",
       "</g>\n",
       "<!-- 3.呼ばれ&#45;&gt;1.人工知能は -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3.呼ばれ&#45;&gt;1.人工知能は</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M285.66,-147.67C260.38,-136.94 224.54,-121.74 196.35,-109.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"197.68,-106.54 187.11,-105.86 194.95,-112.98 197.68,-106.54\"/>\n",
       "</g>\n",
       "<!-- 2.機械学習と -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2.機械学習と</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"317.09\" cy=\"-90\" rx=\"73.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"317.09\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">2.機械学習と</text>\n",
       "</g>\n",
       "<!-- 3.呼ばれ&#45;&gt;2.機械学習と -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3.呼ばれ&#45;&gt;2.機械学習と</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M317.09,-143.7C317.09,-135.98 317.09,-126.71 317.09,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"320.59,-118.1 317.09,-108.1 313.59,-118.1 320.59,-118.1\"/>\n",
       "</g>\n",
       "<!-- 6.ある -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>6.ある</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"377.09\" cy=\"-234\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"377.09\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">6.ある</text>\n",
       "</g>\n",
       "<!-- 6.ある&#45;&gt;3.呼ばれ -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>6.ある&#45;&gt;3.呼ばれ</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M363.17,-216.76C355.71,-208.06 346.36,-197.15 338.03,-187.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"340.51,-184.95 331.34,-179.63 335.19,-189.5 340.51,-184.95\"/>\n",
       "</g>\n",
       "<!-- 5.ものが -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5.ものが</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"466.09\" cy=\"-162\" rx=\"51.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"466.09\" y=\"-158.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">5.ものが</text>\n",
       "</g>\n",
       "<!-- 6.ある&#45;&gt;5.ものが -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>6.ある&#45;&gt;5.ものが</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M396.4,-217.81C408.59,-208.22 424.55,-195.67 438.11,-185\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"440.5,-187.58 446.2,-178.64 436.17,-182.08 440.5,-187.58\"/>\n",
       "</g>\n",
       "<!-- 4.以下のような -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>4.以下のような</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"494.09\" cy=\"-90\" rx=\"85.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"494.09\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">4.以下のような</text>\n",
       "</g>\n",
       "<!-- 5.ものが&#45;&gt;4.以下のような -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5.ものが&#45;&gt;4.以下のような</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M472.86,-144.05C476.02,-136.18 479.84,-126.62 483.37,-117.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"486.71,-118.86 487.18,-108.28 480.21,-116.26 486.71,-118.86\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x18da8419040>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 44. 係り受け木の可視化\n",
    "# 有向グラフの描写：graphviz.Digraph\n",
    "from graphviz import Digraph\n",
    "\n",
    "whole_sentences_chunks = make_whole_sentences_chunks('data/ch05/ai.ja.txt.parsed')\n",
    "organize_srcs(whole_sentences_chunks)\n",
    "erase_symbols(whole_sentences_chunks)\n",
    "\n",
    "def visualize_relation_tree(sentence_chunks):\n",
    "    '''\n",
    "    input:  1文のchunkオブジェクトリスト\n",
    "    output: 係り受け木のDigraphオブジェクト\n",
    "    '''\n",
    "    dg = Digraph(format='png')\n",
    "    for chunk in sentence_chunks:\n",
    "        if chunk.dst == -1:\n",
    "            dg.node(f'{chunk.id}.{chunk.fetch_phrase()}')\n",
    "            continue\n",
    "        # edge: 係り先→係り元\n",
    "        dg.edge(f'{sentence_chunks[chunk.dst].id}.{sentence_chunks[chunk.dst].fetch_phrase()}', f'{chunk.id}.{chunk.fetch_phrase()}')\n",
    "    return dg\n",
    "\n",
    "sixth_relation_tree = visualize_relation_tree(whole_sentences_chunks[6])\n",
    "sixth_relation_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316ba2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display first 10 case patters\n",
      "\n",
      "用いる\tを\n",
      "指す\tを\n",
      "代わる\tに を\n",
      "行う\tて に\n",
      "する\tも\n",
      "述べる\tで に は\n",
      "ある\tが は\n",
      "用いる\tを\n",
      "する\tと を\n",
      "使う\tでも は\n"
     ]
    }
   ],
   "source": [
    "# 45. 動詞の格パターンの抽出\n",
    "# output: '述語\\t格' -> ファイルに保存\n",
    "# 述語: 動詞を含む文節の最左動詞の基本形\n",
    "# 格：述語にかかる助詞(複数の格がある場合、すべてスペース区切りで並べる(辞書順))\n",
    "whole_sentences_chunks = make_whole_sentences_chunks('data/ch05/ai.ja.txt.parsed')\n",
    "organize_srcs(whole_sentences_chunks)\n",
    "erase_symbols(whole_sentences_chunks)\n",
    "\n",
    "def extract_case_pattern(sentence_chunks):\n",
    "    '''\n",
    "    input:  1文のchunkオブジェクトリスト\n",
    "    output: 各パターンのリスト\n",
    "    '''\n",
    "    case_pattern_list = []\n",
    "    for chunk in sentence_chunks:\n",
    "        # 述語がない場合continue\n",
    "        if not chunk.morphs[0].pos == '動詞':\n",
    "            continue\n",
    "        # 係り元の右端が助詞の場合、case_list(格リスト)に追加\n",
    "        case_list = []\n",
    "        for src in chunk.srcs:\n",
    "            src_chunk = sentence_chunks[src]\n",
    "            if src_chunk.morphs[-1].pos == '助詞':\n",
    "                case_list.append(src_chunk.morphs[-1].base)\n",
    "        # case_listが空でない場合、辞書順に並べcase_pattern_listに追加\n",
    "        if case_list:\n",
    "            case_list.sort()\n",
    "            cases = ' '.join(case_list)\n",
    "            case_pattern_list.append(f'{chunk.morphs[0].base}\\t{cases}')\n",
    "    return case_pattern_list\n",
    "\n",
    "whole_case_patterns = []\n",
    "for sentence_chunks in whole_sentences_chunks:\n",
    "    whole_case_patterns += extract_case_pattern(sentence_chunks)\n",
    "\n",
    "with open('data/ch05/case_patterns.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(whole_case_patterns))\n",
    "\n",
    "'''\n",
    "UNIXコマンドと出力\n",
    "cmd: sort case_patterns.txt | uniq -c | sort -nr\n",
    "1. <動詞>よる  <格>に (11)\n",
    "2. <動詞>行う  <格>を (9)\n",
    "3. <動詞>する  <格>と (8)\n",
    "\n",
    "cmd: grep <動詞> case_patterns.txt | sort | uniq -c | sort -nr\n",
    "1. <動詞>行う  <格>を (9)\n",
    "2. <動詞>行う  <格>は を (1)\n",
    "2. <動詞>行う  <格>まで を (1)\n",
    "\n",
    "1. <動詞>なる  <格>に は (3)\n",
    "1. <動詞>なる  <格>が と (3)\n",
    "3. <動詞>なる  <格>に (2)\n",
    "\n",
    "1. <動詞>与える  <格>が に (2)\n",
    "2. <動詞>与える  <格>に は を (1)\n",
    "'''\n",
    "\n",
    "print('display first 10 case patters\\n')\n",
    "for case_pattern in whole_case_patterns[:10]:\n",
    "    print(case_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e66bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display first 10 case patters\n",
      "\n",
      "用いる\tを\t道具を\n",
      "指す\tを\t一分野を\n",
      "代わる\tに を\t人間に 知的行動を\n",
      "行う\tて に\t代わって コンピューターに\n",
      "する\tも\t研究分野とも\n",
      "述べる\tで に は\t解説で 次のように 佐藤理史は\n",
      "ある\tが は\t画像認識等が 応用例は\n",
      "用いる\tを\t記号処理を\n",
      "する\tと を\t主体と 記述を\n",
      "使う\tでも は\t意味あいでも 現在では\n"
     ]
    }
   ],
   "source": [
    "# 46. 動詞の格フレーム情報の抽出\n",
    "# output: '述語\\t格\\t項'\n",
    "# 述語: 動詞を含む文節の最左動詞の基本形\n",
    "# 格：述語にかかる助詞(複数の格がある場合、すべてスペース区切りで並べる(辞書順))\n",
    "# 項：述語にかかる文節の単語列(格と同じ順番で並べる)\n",
    "whole_sentences_chunks = make_whole_sentences_chunks('data/ch05/ai.ja.txt.parsed')\n",
    "organize_srcs(whole_sentences_chunks)\n",
    "erase_symbols(whole_sentences_chunks)\n",
    "\n",
    "def organize_case_arg(case_arg_list):\n",
    "    '''\n",
    "    input:  [[case1, arg1], [case2, arg2],...]\n",
    "    output: cases='case1 case2 ...', args='arg1, arg2' (sorted by case)\n",
    "    '''\n",
    "    case_arg_list.sort(key=lambda x: x[0])\n",
    "    case_list = []\n",
    "    arg_list  = []\n",
    "    for case_arg in case_arg_list:\n",
    "        case_list.append(case_arg[0])\n",
    "        arg_list.append(case_arg[1])\n",
    "    cases = ' '.join(case_list)\n",
    "    args  = ' '.join(arg_list)\n",
    "    return cases, args\n",
    "\n",
    "def extract_case_pattern2(sentence_chunks):\n",
    "    '''\n",
    "    input:  1文のchunkオブジェクトリスト\n",
    "    output: 各パターンのリスト\n",
    "    '''\n",
    "    case_pattern_list = []\n",
    "    for chunk in sentence_chunks:\n",
    "        # 述語がない場合continue\n",
    "        if not chunk.morphs[0].pos == '動詞':\n",
    "            continue\n",
    "        # 係り元の右端が助詞の場合、case_arg_list(格と項の二重リスト)に追加\n",
    "        case_arg_list = []\n",
    "        for src in chunk.srcs:\n",
    "            src_chunk = sentence_chunks[src]\n",
    "            if src_chunk.morphs[-1].pos == '助詞':\n",
    "                case_arg_list.append([src_chunk.morphs[-1].base, src_chunk.fetch_phrase()])\n",
    "        # case_arg_listが空でない場合、辞書順に並べcase_pattern_listに追加\n",
    "        if case_arg_list:\n",
    "            cases, args = organize_case_arg(case_arg_list)\n",
    "            case_pattern_list.append(f'{chunk.morphs[0].base}\\t{cases}\\t{args}')\n",
    "    return case_pattern_list\n",
    "\n",
    "whole_case_patterns2 = []\n",
    "for sentence_chunks in whole_sentences_chunks:\n",
    "    whole_case_patterns2 += extract_case_pattern2(sentence_chunks)\n",
    "\n",
    "print('display first 10 case patters\\n')\n",
    "for case_pattern in whole_case_patterns2[:10]:\n",
    "    print(case_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6a62779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display first 10 case patters\n",
      "\n",
      "行動を代わる\tに の\t人間に 問題解決などの\n",
      "記述をする\tと の\t主体と 知能の\n",
      "注目を集める\tが\tサポートベクターマシンが\n",
      "経験を行う\tに の を\t元に 自らの 学習を\n",
      "学習を行う\tに を\t元に 経験を\n",
      "流行を超える\tの\t一過性の\n",
      "学習を繰り返す\tや\t開発や\n",
      "進化を見せる\tて において は\t加えて 生成技術において 敵対的生成ネットワークは\n",
      "生成を行う\tという\tCreativeAIという\n",
      "開発を行う\tの は\t機械式計算機の エイダ・ラブレスは\n"
     ]
    }
   ],
   "source": [
    "# 47. 機能動詞構文のマイニング\n",
    "# output: '述語\\t格\\t項'\n",
    "# 述語: サ変接続名詞+を+動詞の基本形\n",
    "# 格：述語にかかる助詞(複数の格がある場合、すべてスペース区切りで並べる(辞書順))\n",
    "# 項：述語にかかる文節の単語列(格と同じ順番で並べる)\n",
    "whole_sentences_chunks = make_whole_sentences_chunks('data/ch05/ai.ja.txt.parsed')\n",
    "organize_srcs(whole_sentences_chunks)\n",
    "erase_symbols(whole_sentences_chunks)\n",
    "\n",
    "def extract_case_pattern3(sentence_chunks):\n",
    "    '''\n",
    "    input:  1文のchunkオブジェクトリスト\n",
    "    output: 各パターンのリスト\n",
    "    '''\n",
    "    case_pattern_list = []\n",
    "    for chunk in sentence_chunks:\n",
    "        # 述語がない場合continue\n",
    "        if not chunk.morphs[0].pos == '動詞':\n",
    "            continue\n",
    "        # サ変接続名詞+を+動詞の検索\n",
    "        for src in chunk.srcs:\n",
    "            src_chunk = sentence_chunks[src]\n",
    "            # サ変接続名詞+を+動詞が見つかった場合、格と項を追加する\n",
    "            if len(src_chunk.morphs) < 2:\n",
    "                continue\n",
    "            if (src_chunk.morphs[-1].surface=='を') & (src_chunk.morphs[-2].pos1=='サ変接続'):\n",
    "                # 係り元の右端が助詞の場合、case_arg_list(格と項の二重リスト)に追加\n",
    "                case_arg_list = []\n",
    "                for src2 in chunk.srcs + src_chunk.srcs:\n",
    "                    src2_chunk = sentence_chunks[src2]\n",
    "                    if (src2_chunk.morphs[-1].pos == '助詞') & (src2 != src):\n",
    "                        case_arg_list.append([src2_chunk.morphs[-1].base, src2_chunk.fetch_phrase()])\n",
    "                # case_arg_listが空でない場合、辞書順に並べcase_pattern_listに追加\n",
    "                if case_arg_list:\n",
    "                    cases, args = organize_case_arg(case_arg_list)\n",
    "                    case_pattern_list.append(f'{src_chunk.morphs[-2].surface}{src_chunk.morphs[-1].surface}{chunk.morphs[0].base}\\t{cases}\\t{args}')\n",
    "    return case_pattern_list\n",
    "\n",
    "whole_case_patterns3 = []\n",
    "for sentence_chunks in whole_sentences_chunks:\n",
    "    whole_case_patterns3 += extract_case_pattern3(sentence_chunks)\n",
    "\n",
    "print('display first 10 case patters\\n')\n",
    "for case_pattern in whole_case_patterns3[:10]:\n",
    "    print(case_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a5764fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display first 10 paths to root\n",
      "\n",
      "人工知能 -> 語 -> 研究分野とも -> される\n",
      "じんこうちのう -> 語 -> 研究分野とも -> される\n",
      "AI -> エーアイとは -> 語 -> 研究分野とも -> される\n",
      "計算 -> という -> 道具を -> 用いて -> 研究する -> 計算機科学 -> の -> 一分野を -> 指す -> 語 -> 研究分野とも -> される\n",
      "概念と -> 道具を -> 用いて -> 研究する -> 計算機科学 -> の -> 一分野を -> 指す -> 語 -> 研究分野とも -> される\n",
      "コンピュータ -> という -> 道具を -> 用いて -> 研究する -> 計算機科学 -> の -> 一分野を -> 指す -> 語 -> 研究分野とも -> される\n",
      "知能を -> 研究する -> 計算機科学 -> の -> 一分野を -> 指す -> 語 -> 研究分野とも -> される\n",
      "言語の -> 推論 -> 問題解決などの -> 知的行動を -> 代わって -> 行わせる -> 技術または -> 研究分野とも -> される\n",
      "理解や -> 推論 -> 問題解決などの -> 知的行動を -> 代わって -> 行わせる -> 技術または -> 研究分野とも -> される\n",
      "人間に -> 代わって -> 行わせる -> 技術または -> 研究分野とも -> される\n"
     ]
    }
   ],
   "source": [
    "# 48. 名詞から根へのパスの抽出\n",
    "whole_sentences_chunks = make_whole_sentences_chunks('data/ch05/ai.ja.txt.parsed')\n",
    "organize_srcs(whole_sentences_chunks)\n",
    "erase_symbols(whole_sentences_chunks)\n",
    "\n",
    "def extract_paths_to_root(sentence_chunks):\n",
    "    '''\n",
    "    input:  1文のchunkオブジェクトリスト\n",
    "    output: 名詞から根へのパスのリスト\n",
    "    '''\n",
    "    paths_to_root = []\n",
    "    for chunk in sentence_chunks:\n",
    "        # 葉でない場合continue(葉から根へと探索を進める)\n",
    "        if chunk.srcs:\n",
    "            continue\n",
    "        path_to_root = []\n",
    "        tmp_chunk = chunk   # 注目するノード(葉からルートをたどる)\n",
    "        is_noun = False    # パスの先端に名詞が存在するか\n",
    "        while tmp_chunk.dst != -1:\n",
    "            if tmp_chunk.is_noun() | is_noun:\n",
    "                is_noun = True\n",
    "            if is_noun:\n",
    "                path_to_root.append(tmp_chunk.fetch_phrase())\n",
    "            tmp_chunk = sentence_chunks[tmp_chunk.dst]\n",
    "        if is_noun:\n",
    "            path_to_root.append(tmp_chunk.fetch_phrase())\n",
    "            paths_to_root.append(' -> '.join(path_to_root))\n",
    "    \n",
    "    return paths_to_root\n",
    "\n",
    "whole_paths_to_root = []\n",
    "for sentence_chunks in whole_sentences_chunks:\n",
    "    whole_paths_to_root += extract_paths_to_root(sentence_chunks)\n",
    "\n",
    "print('display first 10 paths to root\\n')\n",
    "for path_to_root in whole_paths_to_root[:10]:\n",
    "    print(path_to_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4852c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display first 10 paths between nouns\n",
      "\n",
      "X | Yのう | 語\n",
      "X | Y -> エーアイとは | 語\n",
      "X | Yとは | 語\n",
      "X | Y -> という -> 道具を -> 用いて -> 研究する -> 計算機科学 -> の -> 一分野を -> 指す | 語\n",
      "X | Yと -> 道具を -> 用いて -> 研究する -> 計算機科学 -> の -> 一分野を -> 指す | 語\n",
      "X | Y -> という -> 道具を -> 用いて -> 研究する -> 計算機科学 -> の -> 一分野を -> 指す | 語\n",
      "X | Yを -> 用いて -> 研究する -> 計算機科学 -> の -> 一分野を -> 指す | 語\n",
      "X | Yを -> 研究する -> 計算機科学 -> の -> 一分野を -> 指す | 語\n",
      "X | Yする -> 計算機科学 -> の -> 一分野を -> 指す | 語\n",
      "X | Y -> の -> 一分野を -> 指す | 語\n"
     ]
    }
   ],
   "source": [
    "# 49. 名詞間の係り受けパスの抽出\n",
    "whole_sentences_chunks = make_whole_sentences_chunks('data/ch05/ai.ja.txt.parsed')\n",
    "organize_srcs(whole_sentences_chunks)\n",
    "erase_symbols(whole_sentences_chunks)\n",
    "\n",
    "def extract_replaced_path_to_root(sentence_chunks, chunk_id, replace):\n",
    "    '''\n",
    "    input:  sentence_chunks: 1文のchunkオブジェクトリスト\n",
    "            chunk_id: path先端のchunk(名詞を含むchunk)のid\n",
    "            replace:  置き換える名詞\n",
    "    output: replaced_path_to_root(list): [[chunk_id1, chunk_phrase1], [chunk_id2, chunk_phrase2],...]\n",
    "    '''\n",
    "    replaced_path_to_root = []\n",
    "    tmp_chunk = sentence_chunks[chunk_id]\n",
    "    cont_flag = 1\n",
    "    while cont_flag:\n",
    "        if tmp_chunk.dst == -1:\n",
    "            cont_flag = 0\n",
    "        if replace:\n",
    "            replaced_path_to_root.append([tmp_chunk.id, tmp_chunk.fetch_replaced_phrase(replace)])\n",
    "            replace = False\n",
    "        else:\n",
    "            replaced_path_to_root.append([tmp_chunk.id, tmp_chunk.fetch_phrase()])\n",
    "        tmp_chunk = sentence_chunks[tmp_chunk.dst]\n",
    "    return replaced_path_to_root\n",
    "\n",
    "def extract_paths_between_nouns(sentence_chunks):\n",
    "    '''\n",
    "    input:  1文のchunkオブジェクトリスト\n",
    "    output: 名詞間の係り受けパスのリスト\n",
    "    '''\n",
    "    paths_between_nouns = []\n",
    "    # [i,j]のリストを作成\n",
    "    nouns_ids = []\n",
    "    for i in range(len(sentence_chunks)):\n",
    "        if sentence_chunks[i].is_noun():\n",
    "            nouns_ids += [[i, j] for j in range(i+1, len(sentence_chunks)) if sentence_chunks[j].is_noun()]\n",
    "    \n",
    "    # 各i,jについてpath_between_nounsを抽出\n",
    "    for i,j in nouns_ids:\n",
    "        # i,jから根へのパスを抽出\n",
    "        i_path = extract_replaced_path_to_root(sentence_chunks, i, 'X')\n",
    "        j_path = extract_replaced_path_to_root(sentence_chunks, j, 'Y')\n",
    "        \n",
    "        # i,jから根へのパスの共通部分を求める\n",
    "        for common_num in range(len(i_path)):\n",
    "            if len(j_path) == common_num:\n",
    "                break\n",
    "            if i_path[-common_num-1][0] != j_path[-common_num-1][0]:\n",
    "                break\n",
    "        \n",
    "        # 共通部分にYが入っている場合   => path_between_nouns: 'X～' ～> 'Y～'\n",
    "        # 共通部分にYが入っていない場合 => path_between_nouns: 'Xの非共通部分' | 'Yの非共通部分' | '共通部分先頭'\n",
    "        if j_path[-common_num][1].startswith('Y'):\n",
    "            path_between_i_j = []\n",
    "            for k in range(len(i_path)-common_num):\n",
    "                path_between_i_j.append(i_path[k][1])\n",
    "            path_between_i_j.append(j_path[0][1])\n",
    "            paths_between_nouns.append(' -> '.join(path_between_i_j))\n",
    "        else:\n",
    "            path_from_i = []\n",
    "            path_from_j = []\n",
    "            for k in range(len(i_path)-common_num):\n",
    "                path_from_i.append(i_path[k][1])\n",
    "            for k in range(len(j_path)-common_num):\n",
    "                path_from_j.append(j_path[k][1])\n",
    "            paths_between_nouns.append(' -> '.join(path_from_i) + ' | ' + ' -> '.join(path_from_j) + ' | ' + i_path[-common_num][1])\n",
    "        \n",
    "    return paths_between_nouns\n",
    "\n",
    "whole_paths_between_nouns = []\n",
    "for sentence_chunks in whole_sentences_chunks:\n",
    "    whole_paths_between_nouns += extract_paths_between_nouns(sentence_chunks)\n",
    "\n",
    "print('display first 10 paths between nouns\\n')\n",
    "for path_between_nouns in whole_paths_between_nouns[:10]:\n",
    "    print(path_between_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3df9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
